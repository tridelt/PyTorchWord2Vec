{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes a textfile according with skip-gram algorithm\n",
    "# returns a list in the following format [[7,4],[42,44], ...]\n",
    "from preprocessing import Preprocess\n",
    "from utils import createWordPairs\n",
    "import pickle\n",
    "\n",
    "# Variables: \n",
    "#     threshold: how many emojis count as a sequence\n",
    "#     window_size: determine how far to the left and right of center_word the skip-gram algo forms word_pairs\n",
    "threshold = 2\n",
    "window_size = 8\n",
    "\n",
    "indexes = Preprocess('./data/initializationSet.txt', threshold)\n",
    "trainingCorpus = Preprocess('./data/trainingSet.txt', threshold)\n",
    "validationCorpus = Preprocess('./data/validationSet.txt', threshold)\n",
    "trainingPairs = createWordPairs(indexes, trainingCorpus, window_size)\n",
    "validationPairs = createWordPairs(indexes, validationCorpus, window_size)\n",
    "\n",
    "indexes_out = open(\"./Preprocess_Files/indexes.pickle\",\"wb\")\n",
    "trainingCorpus_out = open(\"./Preprocess_Files/trainingCorpus.pickle\",\"wb\")\n",
    "validationCorpus_out = open(\"./Preprocess_Files/validationCorpus.pickle\",\"wb\")\n",
    "trainingPairs_out = open(\"./Preprocess_Files/trainingPairs.pickle\",\"wb\")\n",
    "validationPairs_out = open(\"./Preprocess_Files/validationPairs.pickle\",\"wb\")\n",
    "\n",
    "pickle.dump(indexes, indexes_out)\n",
    "pickle.dump(trainingCorpus, trainingCorpus_out)\n",
    "pickle.dump(validationCorpus, validationCorpus_out)\n",
    "pickle.dump(trainingPairs, trainingPairs_out)\n",
    "pickle.dump(validationPairs, validationPairs_out)\n",
    "\n",
    "indexes_out.close()\n",
    "trainingCorpus_out.close()\n",
    "validationCorpus_out.close()\n",
    "trainingPairs_out.close()\n",
    "validationPairs_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sampler import ImbalancedDatasetSampler\n",
    "from dataset import LoadedDataSet\n",
    "from model import Word2Vec\n",
    "from utils import EarlyStopping\n",
    "\n",
    "# importing preprocessing_files\n",
    "indexes_in = open(\"./Preprocess_Files/indexes.pickle\",\"rb\")\n",
    "trainingCorpus_in = open(\"./Preprocess_Files/trainingCorpus.pickle\",\"rb\")\n",
    "validationCorpus_in = open(\"./Preprocess_Files/validationCorpus.pickle\",\"rb\")\n",
    "trainingPairs_in = open(\"./Preprocess_Files/trainingPairs.pickle\",\"rb\")\n",
    "validationPairs_in = open(\"./Preprocess_Files/validationPairs.pickle\",\"rb\")\n",
    "indexes = pickle.load(indexes_in)\n",
    "trainingCorpus = pickle.load(trainingCorpus_in)\n",
    "validationCorpus = pickle.load(validationCorpus_in)\n",
    "trainingPairs = pickle.load(trainingPairs_in)\n",
    "validationPairs = pickle.load(validationPairs_in)\n",
    "indexes_in.close()\n",
    "trainingCorpus_in.close()\n",
    "validationCorpus_in.close()\n",
    "trainingPairs_in.close()\n",
    "validationPairs_in.close()\n",
    "\n",
    "# HyperParams\n",
    "dimensionSize = 300\n",
    "num_epochs = 600\n",
    "lr = 0.5\n",
    "batchSize = 150\n",
    "patience = 5\n",
    "save_name = \"noNumpy.w2v\"\n",
    "verbose = True\n",
    "\n",
    "trainingDataset = LoadedDataSet(trainingPairs)\n",
    "sampler = ImbalancedDatasetSampler(trainingDataset)\n",
    "\n",
    "trainingLoader = DataLoader(trainingDataset, batchSize, sampler=sampler) \n",
    "validationDataset = LoadedDataSet(validationPairs)\n",
    "validationLoader = DataLoader(validationDataset, batchSize, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[  1/600] train_loss: 3.86248 valid_loss: 4.47258\n",
      "Validation loss decreased (inf --> 4.472579).  Saving model ...\n",
      "0.5\n",
      "[  2/600] train_loss: 3.25538 valid_loss: 4.39424\n",
      "Validation loss decreased (4.472579 --> 4.394238).  Saving model ...\n",
      "0.5\n",
      "[  3/600] train_loss: 3.16392 valid_loss: 4.36100\n",
      "Validation loss decreased (4.394238 --> 4.361001).  Saving model ...\n",
      "0.5\n",
      "[  4/600] train_loss: 3.12141 valid_loss: 4.36275\n",
      "EarlyStopping counter: 1 out of 5\n",
      "0.05\n",
      "[  5/600] train_loss: 3.04837 valid_loss: 4.30353\n",
      "Validation loss decreased (4.361001 --> 4.303529).  Saving model ...\n",
      "0.05\n",
      "[  6/600] train_loss: 3.02666 valid_loss: 4.29724\n",
      "Validation loss decreased (4.303529 --> 4.297238).  Saving model ...\n",
      "0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1e5b9978e39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# best trainable after extending the data_rate_limit..\n",
    "# use terminal cmd w/ MAC: \"jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\"\n",
    "\n",
    "model = Word2Vec(indexes.vocabulary_size, dimensionSize)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "# Variables for Scheduler:\n",
    "# 3,6,10,20\n",
    "# 5,10,40              52\n",
    "# comparable with much larger set? can I make conclusions here?\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3,6,10,20], gamma=0.1)\n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience, verbose, save_name)\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "\n",
    "for e in range(1, num_epochs + 1):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])    \n",
    "#         ###################\n",
    "#         # train the model #\n",
    "#         ###################     \n",
    "    for i, (data, target) in enumerate(trainingLoader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.forward(data, target)       \n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "  \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "    for i, (data, target) in enumerate(validationLoader): \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model.forward(data, target)       \n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    avg_train_losses.append(np.average(train_losses))\n",
    "    avg_valid_losses.append(np.average(valid_losses))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "###################################################    \n",
    "#   visual Update regarding the current epoch   #\n",
    "###################################################    \n",
    "    epoch_len = len(str(num_epochs))\n",
    "    print_msg = (f'[{e:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +\n",
    "                 f'train_loss: {np.average(train_losses):.5f} ' +\n",
    "                 f'valid_loss: {np.average(valid_losses):.5f}')\n",
    "    print(print_msg)\n",
    "    \n",
    "        ##################\n",
    "        # Early Stopping #\n",
    "        ##################\n",
    "    early_stopping(np.average(valid_losses), model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"EARLY STOPPING!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Loss and the Early Stopping Checkpoint¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
    "plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = avg_valid_losses.index(min(avg_valid_losses))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(3.5, 7) # consistent scale\n",
    "plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('./SOFTMAX_trained/plots/loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING & EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between 🇬🇧 and 🇺🇸 could not be computed.\n",
      "the cosine similarity between 🏅 and 🇺🇸 could not be computed.\n",
      "the cosine similarity between 🇺🇸 and ❤ could not be computed.\n",
      "the cosine similarity between 🇺🇸 and 💥 could not be computed.\n",
      "the cosine similarity between 🎤 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🇳🇬 and 📲 could not be computed.\n",
      "the cosine similarity between 👇 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🎧 and 🇳🇬 could not be computed.\n",
      "the cosine similarity between 🇳🇬 and 🎶 could not be computed.\n",
      "the cosine similarity between 👏 and ↪ could not be computed.\n",
      "\n",
      "mein Spearman: 0.46396928155487416\n",
      "sein Spearman: 0.7590552598973165\n",
      "mein MAE ist 0.34152071209826385\n",
      "sein MAE ist 0.24206827309236947\n",
      "mein MSE ist 0.16214684391402837\n",
      "sein MSE ist 0.08023220381526104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype <U4 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:248: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<evaluation.Metrics at 0x125284198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads the weights of a saved model and calculates and prints the following metrics: SpearManRank, MSE, MAE \n",
    "# look @ the data with TensorBoardX \"tensorboard --logdir runs\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from evaluation import Metrics\n",
    "\n",
    "loadedModel = torch.load(os.path.join(\"SOFTMAX_trained\", \"noNumpy.w2v\"))\n",
    "Metrics(loadedModel.weight.data.cpu().numpy(), indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadedModel.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.942554"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1111, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pp.figure(figsize=(10,10))\n",
    "\n",
    "pp.plot(model[0])\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 2.0000e+00, 3.0000e+00, 3.0000e+00, 3.0000e+00,\n",
       "        1.0000e+00, 3.0000e+00, 2.0000e+00, 3.0000e+00, 3.0000e+00,\n",
       "        1.0000e+00, 2.0000e+00, 5.0000e+00, 1.0000e+00, 4.0000e+00,\n",
       "        4.0000e+00, 2.0000e+00, 4.0000e+00, 8.0000e+00, 5.0000e+00,\n",
       "        7.0000e+00, 5.0000e+00, 7.0000e+00, 1.1000e+01, 4.0000e+00,\n",
       "        1.6000e+01, 1.0000e+01, 1.0000e+01, 1.4000e+01, 1.1000e+01,\n",
       "        1.6000e+01, 1.9000e+01, 1.6000e+01, 1.8000e+01, 2.3000e+01,\n",
       "        3.3000e+01, 2.1000e+01, 2.2000e+01, 2.8000e+01, 3.1000e+01,\n",
       "        3.5000e+01, 4.9000e+01, 4.6000e+01, 5.8000e+01, 5.6000e+01,\n",
       "        6.4000e+01, 6.8000e+01, 7.5000e+01, 6.5000e+01, 8.2000e+01,\n",
       "        8.5000e+01, 8.8000e+01, 9.7000e+01, 1.1600e+02, 1.0300e+02,\n",
       "        1.1800e+02, 1.5500e+02, 1.4300e+02, 1.5100e+02, 1.7900e+02,\n",
       "        1.7900e+02, 1.9000e+02, 2.3800e+02, 2.2400e+02, 2.2500e+02,\n",
       "        2.6400e+02, 2.6400e+02, 2.8300e+02, 3.1400e+02, 3.3900e+02,\n",
       "        3.4500e+02, 3.9800e+02, 4.0800e+02, 4.6700e+02, 4.4700e+02,\n",
       "        5.1400e+02, 5.4100e+02, 5.8100e+02, 6.1800e+02, 6.5100e+02,\n",
       "        7.5400e+02, 7.0300e+02, 7.5700e+02, 8.2200e+02, 9.2200e+02,\n",
       "        8.7300e+02, 9.8000e+02, 1.0040e+03, 1.1000e+03, 1.2080e+03,\n",
       "        1.2550e+03, 1.3300e+03, 1.3690e+03, 1.4550e+03, 1.5170e+03,\n",
       "        1.5220e+03, 1.7230e+03, 1.7990e+03, 1.9120e+03, 1.9750e+03,\n",
       "        2.1640e+03, 2.1430e+03, 2.2920e+03, 2.3370e+03, 2.4640e+03,\n",
       "        2.5570e+03, 2.5950e+03, 2.7610e+03, 2.9770e+03, 3.0370e+03,\n",
       "        3.1600e+03, 3.3440e+03, 3.4520e+03, 3.5250e+03, 3.6620e+03,\n",
       "        3.8090e+03, 3.8810e+03, 3.9820e+03, 3.9870e+03, 4.1350e+03,\n",
       "        4.1510e+03, 4.4740e+03, 4.5570e+03, 4.4770e+03, 4.6980e+03,\n",
       "        4.7290e+03, 4.9750e+03, 4.7890e+03, 4.9660e+03, 4.9840e+03,\n",
       "        4.9270e+03, 5.1100e+03, 5.1490e+03, 1.0765e+04, 5.1620e+03,\n",
       "        5.0390e+03, 5.0050e+03, 4.9940e+03, 5.0100e+03, 4.8520e+03,\n",
       "        4.9230e+03, 4.8810e+03, 4.6940e+03, 4.5770e+03, 4.3960e+03,\n",
       "        4.4700e+03, 4.5210e+03, 4.2060e+03, 4.2980e+03, 4.0480e+03,\n",
       "        3.9490e+03, 3.9530e+03, 3.7150e+03, 3.7070e+03, 3.4480e+03,\n",
       "        3.4460e+03, 3.2930e+03, 3.2220e+03, 3.0470e+03, 2.9780e+03,\n",
       "        2.7810e+03, 2.6900e+03, 2.6130e+03, 2.4830e+03, 2.4040e+03,\n",
       "        2.2650e+03, 2.1920e+03, 2.0790e+03, 1.9580e+03, 1.8750e+03,\n",
       "        1.7860e+03, 1.6500e+03, 1.6550e+03, 1.5060e+03, 1.3890e+03,\n",
       "        1.3980e+03, 1.2760e+03, 1.2550e+03, 1.1750e+03, 1.0690e+03,\n",
       "        1.0880e+03, 9.7600e+02, 9.1300e+02, 8.7000e+02, 8.2300e+02,\n",
       "        7.6200e+02, 7.0400e+02, 6.9400e+02, 6.4500e+02, 6.0200e+02,\n",
       "        6.1900e+02, 5.0300e+02, 4.6900e+02, 5.0400e+02, 4.5700e+02,\n",
       "        4.2000e+02, 3.5600e+02, 4.0100e+02, 3.2500e+02, 3.2400e+02,\n",
       "        2.8800e+02, 2.7000e+02, 2.4900e+02, 2.5400e+02, 2.2400e+02,\n",
       "        1.9600e+02, 1.6800e+02, 1.6700e+02, 1.5700e+02, 1.2900e+02,\n",
       "        1.2100e+02, 1.2200e+02, 1.2200e+02, 1.1800e+02, 1.1100e+02,\n",
       "        7.1000e+01, 9.4000e+01, 9.2000e+01, 5.9000e+01, 5.9000e+01,\n",
       "        4.8000e+01, 5.4000e+01, 6.0000e+01, 5.1000e+01, 4.1000e+01,\n",
       "        3.4000e+01, 3.4000e+01, 3.6000e+01, 3.5000e+01, 2.9000e+01,\n",
       "        2.1000e+01, 3.1000e+01, 2.0000e+01, 2.1000e+01, 2.3000e+01,\n",
       "        1.4000e+01, 1.3000e+01, 1.2000e+01, 1.6000e+01, 1.4000e+01,\n",
       "        1.7000e+01, 1.1000e+01, 1.2000e+01, 5.0000e+00, 6.0000e+00,\n",
       "        1.1000e+01, 5.0000e+00, 4.0000e+00, 5.0000e+00, 2.0000e+00,\n",
       "        8.0000e+00, 4.0000e+00, 2.0000e+00, 0.0000e+00, 5.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 2.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "        0.0000e+00, 3.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        2.0000e+00, 0.0000e+00, 2.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([-656.69464  , -652.2767   , -647.8587   , -643.4408   ,\n",
       "        -639.0228   , -634.60486  , -630.1869   , -625.7689   ,\n",
       "        -621.351    , -616.93304  , -612.5151   , -608.0971   ,\n",
       "        -603.67914  , -599.2612   , -594.84326  , -590.4253   ,\n",
       "        -586.0073   , -581.58936  , -577.17145  , -572.7535   ,\n",
       "        -568.3355   , -563.91754  , -559.4996   , -555.08167  ,\n",
       "        -550.6637   , -546.2457   , -541.82776  , -537.4098   ,\n",
       "        -532.9919   , -528.5739   , -524.15594  , -519.738    ,\n",
       "        -515.32     , -510.90207  , -506.48413  , -502.06616  ,\n",
       "        -497.64822  , -493.23026  , -488.8123   , -484.39435  ,\n",
       "        -479.97638  , -475.55844  , -471.14047  , -466.7225   ,\n",
       "        -462.30457  , -457.8866   , -453.46866  , -449.0507   ,\n",
       "        -444.63272  , -440.21478  , -435.7968   , -431.37888  ,\n",
       "        -426.9609   , -422.54297  , -418.125    , -413.70703  ,\n",
       "        -409.2891   , -404.87112  , -400.4532   , -396.03522  ,\n",
       "        -391.61725  , -387.1993   , -382.78134  , -378.3634   ,\n",
       "        -373.94543  , -369.52747  , -365.10953  , -360.69156  ,\n",
       "        -356.27362  , -351.85565  , -347.43768  , -343.01974  ,\n",
       "        -338.60178  , -334.18384  , -329.76587  , -325.3479   ,\n",
       "        -320.92996  , -316.512    , -312.09406  , -307.6761   ,\n",
       "        -303.25812  , -298.84018  , -294.4222   , -290.00427  ,\n",
       "        -285.5863   , -281.16837  , -276.7504   , -272.33243  ,\n",
       "        -267.9145   , -263.49652  , -259.07858  , -254.66061  ,\n",
       "        -250.24266  , -245.8247   , -241.40674  , -236.98878  ,\n",
       "        -232.57083  , -228.15288  , -223.73492  , -219.31696  ,\n",
       "        -214.899    , -210.48105  , -206.0631   , -201.64514  ,\n",
       "        -197.22717  , -192.80922  , -188.39127  , -183.97331  ,\n",
       "        -179.55536  , -175.1374   , -170.71944  , -166.30148  ,\n",
       "        -161.88353  , -157.46558  , -153.04762  , -148.62965  ,\n",
       "        -144.2117   , -139.79375  , -135.3758   , -130.95784  ,\n",
       "        -126.53988  , -122.12192  , -117.703964 , -113.28601  ,\n",
       "        -108.86805  , -104.450096 , -100.03214  ,  -95.61418  ,\n",
       "         -91.19623  ,  -86.77827  ,  -82.36031  ,  -77.94236  ,\n",
       "         -73.5244   ,  -69.106445 ,  -64.68849  ,  -60.27053  ,\n",
       "         -55.852577 ,  -51.43462  ,  -47.016663 ,  -42.598705 ,\n",
       "         -38.18075  ,  -33.762794 ,  -29.344837 ,  -24.926882 ,\n",
       "         -20.508924 ,  -16.09097  ,  -11.673013 ,   -7.255056 ,\n",
       "          -2.8370998,    1.5808567,    5.998813 ,   10.416769 ,\n",
       "          14.834725 ,   19.252682 ,   23.670639 ,   28.088594 ,\n",
       "          32.50655  ,   36.924507 ,   41.342464 ,   45.76042  ,\n",
       "          50.178375 ,   54.596333 ,   59.01429  ,   63.432247 ,\n",
       "          67.850204 ,   72.26816  ,   76.68611  ,   81.10407  ,\n",
       "          85.522026 ,   89.93999  ,   94.35794  ,   98.775894 ,\n",
       "         103.193855 ,  107.61181  ,  112.02976  ,  116.44772  ,\n",
       "         120.86568  ,  125.28364  ,  129.70158  ,  134.11955  ,\n",
       "         138.5375   ,  142.95546  ,  147.37341  ,  151.79137  ,\n",
       "         156.20934  ,  160.62729  ,  165.04524  ,  169.4632   ,\n",
       "         173.88115  ,  178.29912  ,  182.71707  ,  187.13503  ,\n",
       "         191.55298  ,  195.97093  ,  200.38889  ,  204.80685  ,\n",
       "         209.22481  ,  213.64276  ,  218.06071  ,  222.47867  ,\n",
       "         226.89664  ,  231.31459  ,  235.73254  ,  240.1505   ,\n",
       "         244.56845  ,  248.98642  ,  253.40437  ,  257.82233  ,\n",
       "         262.2403   ,  266.65823  ,  271.0762   ,  275.49414  ,\n",
       "         279.9121   ,  284.33008  ,  288.74802  ,  293.166    ,\n",
       "         297.58392  ,  302.0019   ,  306.41983  ,  310.8378   ,\n",
       "         315.25577  ,  319.6737   ,  324.09167  ,  328.5096   ,\n",
       "         332.92758  ,  337.34555  ,  341.7635   ,  346.18146  ,\n",
       "         350.5994   ,  355.01736  ,  359.43533  ,  363.85327  ,\n",
       "         368.27124  ,  372.68918  ,  377.10715  ,  381.52512  ,\n",
       "         385.94305  ,  390.36102  ,  394.77896  ,  399.19693  ,\n",
       "         403.6149   ,  408.03284  ,  412.4508   ,  416.86874  ,\n",
       "         421.2867   ,  425.70468  ,  430.12262  ,  434.5406   ,\n",
       "         438.95853  ,  443.3765   ,  447.79446  ,  452.2124   ,\n",
       "         456.63037  ,  461.0483   ,  465.46628  ,  469.88422  ,\n",
       "         474.3022   ,  478.72015  ,  483.1381   ,  487.55606  ,\n",
       "         491.974    ,  496.39197  ,  500.80994  ,  505.22787  ,\n",
       "         509.64584  ,  514.0638   ,  518.48175  ,  522.8997   ,\n",
       "         527.3177   ,  531.7356   ,  536.15356  ,  540.57153  ,\n",
       "         544.9895   ,  549.4075   ,  553.8254   ,  558.24335  ,\n",
       "         562.6613   ,  567.0793   ,  571.49725  ,  575.91516  ,\n",
       "         580.3331   ,  584.7511   ,  589.16907  ,  593.58704  ,\n",
       "         598.00494  ,  602.4229   ,  606.8409   ,  611.25885  ,\n",
       "         615.6768   ,  620.0947   ,  624.5127   ,  628.93066  ,\n",
       "         633.34863  ,  637.7666   ,  642.1845   ,  646.6025   ,\n",
       "         651.02045  ,  655.4384   ,  659.8564   ,  664.2743   ,\n",
       "         668.69226  ,  673.1102   ,  677.5282   ,  681.9461   ,\n",
       "         686.3641   ,  690.78204  ,  695.2      ,  699.618    ,\n",
       "         704.0359   ,  708.45386  ,  712.8718   ,  717.2898   ,\n",
       "         721.70776  ,  726.1257   ,  730.54364  ,  734.9616   ,\n",
       "         739.3796   ,  743.79755  ,  748.21545  ,  752.6334   ,\n",
       "         757.0514   ,  761.46936  ,  765.8873   ,  770.30524  ,\n",
       "         774.7232   ,  779.1412   ,  783.55914  ,  787.9771   ,\n",
       "         792.395    ,  796.813    ,  801.23096  ,  805.6489   ,\n",
       "         810.0669   ,  814.4848   ,  818.9028   ,  823.32074  ],\n",
       "       dtype=float32),\n",
       " <a list of 335 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(weights, bins='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
