{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ðŸ†”'\n",
      "'ðŸ›¢'\n",
      "'ðŸ›¢'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”¢'\n",
      "'â—¾'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'â¬œ'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'â–ª'\n",
      "'â™Œ'\n",
      "'ðŸ” '\n",
      "'â–«'\n",
      "'â–«'\n",
      "'ðŸ”•'\n",
      "'â—¾'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'ðŸ”•'\n",
      "'â—¼'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â—¾'\n",
      "'âŽ'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â—¼'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸ“¤'\n",
      "'âŽ'\n",
      "'â–«'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'âŽ'\n",
      "'â™‹'\n",
      "'â—¼'\n",
      "'â—¾'\n",
      "'ðŸ’ˆ'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'â¯'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸŽ¦'\n",
      "'â¯'\n",
      "'âŽ'\n",
      "'ðŸ’ˆ'\n",
      "'â¯'\n",
      "'â—¼'\n",
      "'ðŸ—¾'\n",
      "'â¬œ'\n",
      "'â–«'\n",
      "'âŽ'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸ”¢'\n",
      "'â¯'\n",
      "'ðŸ“†'\n",
      "'â˜£'\n",
      "'â–ª'\n",
      "'â¯'\n",
      "'â–«'\n",
      "'â—¾'\n",
      "'â˜£'\n",
      "'âŽ'\n",
      "'ðŸ”¯'\n",
      "'âŽ'\n",
      "'ðŸ’ˆ'\n",
      "'â—¾'\n",
      "'âŽ'\n",
      "'âŽ'\n",
      "'â–«'\n",
      "'â—¾'\n",
      "'â–«'\n",
      "'âŽ'\n",
      "'â†ª'\n",
      "'ðŸ”–'\n",
      "'â†ª'\n",
      "'â—¼'\n",
      "'â–«'\n",
      "'â¯'\n",
      "'â–«'\n",
      "'â¯'\n",
      "'âŽ'\n",
      "'â–«'\n",
      "'â¯'\n",
      "'â—¾'\n",
      "'ðŸŽ¦'\n",
      "'â—¾'\n",
      "'â–«'\n",
      "'â—¾'\n",
      "'â–ª'\n",
      "'ðŸŽ¦'\n",
      "'â—¾'\n",
      "'â¯'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'ðŸ“„'\n",
      "'ðŸ“„'\n",
      "'ðŸ“„'\n",
      "'â–«'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸŽ¦'\n",
      "'â¯'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸš‡'\n",
      "'ðŸŽ¦'\n",
      "'â¯'\n",
      "'ðŸ“›'\n",
      "'â˜£'\n",
      "'â¯'\n",
      "'ðŸ“›'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸŽ¦'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'âŽ'\n",
      "'â—¾'\n",
      "'ðŸ”•'\n",
      "'ðŸ“›'\n",
      "'ðŸ“†'\n",
      "'ðŸ“†'\n",
      "'ðŸ“†'\n",
      "'ã€°'\n",
      "'ðŸ“›'\n",
      "'ðŸ”•'\n",
      "'ã€°'\n",
      "'â—¾'\n",
      "'ðŸ”¢'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'ðŸ”¢'\n",
      "'â¯'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'ðŸ›'\n",
      "'â—¾'\n",
      "'ðŸ”–'\n",
      "'ðŸ”–'\n",
      "'ðŸˆ·'\n",
      "'ãŠ™'\n",
      "'ðŸˆ‚'\n",
      "'â–«'\n",
      "'ðŸš‹'\n",
      "'ðŸš‡'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ“†'\n",
      "'â–«'\n",
      "'ðŸ’ˆ'\n",
      "'â–«'\n",
      "'â™‰'\n",
      "'â—¼'\n",
      "'â—¼'\n",
      "'â—¼'\n",
      "'â¯'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ‘²'\n",
      "'â†©'\n",
      "'â¯'\n",
      "'ðŸ”•'\n",
      "'ðŸ“†'\n",
      "'ðŸ“†'\n",
      "'ðŸ“†'\n",
      "'â—¾'\n",
      "'â˜£'\n",
      "'â–«'\n",
      "'â—¾'\n",
      "'â—¾'\n",
      "'â¯'\n",
      "'ðŸ—¾'\n",
      "'ðŸ“‹'\n",
      "'ðŸš‰'\n",
      "'ðŸš‰'\n",
      "'ðŸš‡'\n",
      "'ðŸº'\n",
      "'ðŸº'\n",
      "'ðŸš±'\n",
      "'â¯'\n",
      "'âŽ'\n",
      "'ðŸŽ¦'\n",
      "'ðŸŽ¦'\n",
      "'ðŸŽ¦'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'âŽ'\n",
      "'â¯'\n",
      "'âŽ'\n",
      "'â¯'\n",
      "'â–«'\n",
      "'ðŸ›„'\n",
      "'âŽ'\n",
      "'ðŸ›„'\n",
      "'ðŸ›„'\n",
      "'â¯'\n",
      "'ðŸ›„'\n",
      "'ðŸ“'\n",
      "'ðŸ›„'\n",
      "'ðŸ›„'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸ’ˆ'\n",
      "'â¯'\n",
      "'ðŸ”³'\n",
      "'ðŸ”³'\n",
      "'ðŸ›„'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'â˜£'\n",
      "'ðŸ“„'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ”€'\n",
      "'ðŸ”€'\n",
      "'âŽ'\n",
      "'ðŸ”¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‡'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ“†'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'ðŸ“'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸ—œ'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ”–'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'ðŸš‹'\n",
      "'ðŸŽ¦'\n",
      "'ðŸŽ¦'\n",
      "'ðŸŽ¦'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ†”'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ“†'\n",
      "'ðŸš‹'\n",
      "'â™Œ'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â™Œ'\n",
      "'âŽ'\n",
      "'ðŸš‡'\n",
      "'ðŸŽ¦'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'ðŸš‹'\n",
      "'â–«'\n",
      "'âŽ'\n",
      "'ðŸ›„'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‡'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'â„¹'\n",
      "'ðŸ›„'\n",
      "'â˜£'\n",
      "'ðŸ”'\n",
      "'ðŸ”'\n",
      "'ðŸ›„'\n",
      "'ðŸ”²'\n",
      "'âž¿'\n",
      "'ðŸŽ¦'\n",
      "'ðŸŽ¦'\n",
      "'ðŸŽ¦'\n",
      "'ðŸ›„'\n",
      "'â—¾'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸ“„'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'â™‹'\n",
      "'â™‹'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ“„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â†•'\n",
      "'â†•'\n",
      "'ðŸ’ˆ'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ”¢'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸš‹'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ’ˆ'\n",
      "'â†•'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ”¯'\n",
      "'ðŸ”¯'\n",
      "'ðŸš‡'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'ðŸ›'\n",
      "'ðŸ›„'\n",
      "'ðŸ’ˆ'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'ðŸ›„'\n",
      "'ðŸš‹'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'ðŸ“†'\n",
      "'ðŸ“†'\n",
      "'ðŸ“†'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'ðŸ’ˆ'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸš‹'\n",
      "'âŽ'\n",
      "'ðŸ“„'\n",
      "'ðŸš‹'\n",
      "'â¯'\n",
      "'â¯'\n",
      "'ðŸ’ˆ'\n",
      "'â¯'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n",
      "'â–ª'\n"
     ]
    }
   ],
   "source": [
    "# processes a textfile according with skip-gram algorithm\n",
    "# returns a list in the following format [[7,4],[42,44], ...]\n",
    "from preprocessing import Preprocess\n",
    "from utils import createWordPairs\n",
    "\n",
    "# Variables: \n",
    "#     threshold: how many emojis count as a sequence\n",
    "#     window_size: determine how far to the left and right of center_word the skip-gram algo forms word_pairs\n",
    "threshold = 2\n",
    "window_size = 8\n",
    "\n",
    "indexes = Preprocess('./data/initializationSet.txt', threshold)\n",
    "trainingCorpus = Preprocess('./data/trainingSet.txt', threshold)\n",
    "validationCorpus = Preprocess('./data/validationSet.txt', threshold)\n",
    "\n",
    "trainingPairs = createWordPairs(indexes, trainingCorpus, window_size)\n",
    "validationPairs = createWordPairs(indexes, validationCorpus, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/600] train_loss: 6.93868 valid_loss: 6.87598\n",
      "Validation loss decreased (inf --> 6.875985).  Saving model ...\n",
      "[  2/600] train_loss: 6.79089 valid_loss: 6.74094\n",
      "Validation loss decreased (6.875985 --> 6.740937).  Saving model ...\n",
      "[  3/600] train_loss: 6.64532 valid_loss: 6.60813\n",
      "Validation loss decreased (6.740937 --> 6.608127).  Saving model ...\n",
      "[  4/600] train_loss: 6.50260 valid_loss: 6.47888\n",
      "Validation loss decreased (6.608127 --> 6.478880).  Saving model ...\n",
      "[  5/600] train_loss: 6.36526 valid_loss: 6.35596\n",
      "Validation loss decreased (6.478880 --> 6.355960).  Saving model ...\n",
      "[  6/600] train_loss: 6.23639 valid_loss: 6.24196\n",
      "Validation loss decreased (6.355960 --> 6.241956).  Saving model ...\n",
      "[  7/600] train_loss: 6.11803 valid_loss: 6.13722\n",
      "Validation loss decreased (6.241956 --> 6.137225).  Saving model ...\n",
      "[  8/600] train_loss: 6.00968 valid_loss: 6.04052\n",
      "Validation loss decreased (6.137225 --> 6.040525).  Saving model ...\n",
      "[  9/600] train_loss: 5.90989 valid_loss: 5.95082\n",
      "Validation loss decreased (6.040525 --> 5.950820).  Saving model ...\n",
      "[ 10/600] train_loss: 5.81776 valid_loss: 5.86721\n",
      "Validation loss decreased (5.950820 --> 5.867211).  Saving model ...\n",
      "[ 11/600] train_loss: 5.73221 valid_loss: 5.78951\n",
      "Validation loss decreased (5.867211 --> 5.789510).  Saving model ...\n",
      "[ 12/600] train_loss: 5.65293 valid_loss: 5.71734\n",
      "Validation loss decreased (5.789510 --> 5.717339).  Saving model ...\n",
      "[ 13/600] train_loss: 5.57935 valid_loss: 5.65033\n",
      "Validation loss decreased (5.717339 --> 5.650326).  Saving model ...\n",
      "[ 14/600] train_loss: 5.51118 valid_loss: 5.58826\n",
      "Validation loss decreased (5.650326 --> 5.588264).  Saving model ...\n",
      "[ 15/600] train_loss: 5.44796 valid_loss: 5.53072\n",
      "Validation loss decreased (5.588264 --> 5.530717).  Saving model ...\n",
      "[ 16/600] train_loss: 5.38915 valid_loss: 5.47725\n",
      "Validation loss decreased (5.530717 --> 5.477251).  Saving model ...\n",
      "[ 17/600] train_loss: 5.33434 valid_loss: 5.42758\n",
      "Validation loss decreased (5.477251 --> 5.427582).  Saving model ...\n",
      "[ 18/600] train_loss: 5.28319 valid_loss: 5.38111\n",
      "Validation loss decreased (5.427582 --> 5.381113).  Saving model ...\n",
      "[ 19/600] train_loss: 5.23523 valid_loss: 5.33769\n",
      "Validation loss decreased (5.381113 --> 5.337686).  Saving model ...\n",
      "[ 20/600] train_loss: 5.19019 valid_loss: 5.29688\n",
      "Validation loss decreased (5.337686 --> 5.296881).  Saving model ...\n",
      "[ 21/600] train_loss: 5.14783 valid_loss: 5.25858\n",
      "Validation loss decreased (5.296881 --> 5.258578).  Saving model ...\n",
      "[ 22/600] train_loss: 5.10782 valid_loss: 5.22242\n",
      "Validation loss decreased (5.258578 --> 5.222420).  Saving model ...\n",
      "[ 23/600] train_loss: 5.07001 valid_loss: 5.18833\n",
      "Validation loss decreased (5.222420 --> 5.188328).  Saving model ...\n",
      "[ 24/600] train_loss: 5.03424 valid_loss: 5.15600\n",
      "Validation loss decreased (5.188328 --> 5.155999).  Saving model ...\n",
      "[ 25/600] train_loss: 5.00043 valid_loss: 5.12538\n",
      "Validation loss decreased (5.155999 --> 5.125375).  Saving model ...\n",
      "[ 26/600] train_loss: 4.96818 valid_loss: 5.09635\n",
      "Validation loss decreased (5.125375 --> 5.096352).  Saving model ...\n",
      "[ 27/600] train_loss: 4.93763 valid_loss: 5.06860\n",
      "Validation loss decreased (5.096352 --> 5.068597).  Saving model ...\n",
      "[ 28/600] train_loss: 4.90852 valid_loss: 5.04226\n",
      "Validation loss decreased (5.068597 --> 5.042264).  Saving model ...\n",
      "[ 29/600] train_loss: 4.88081 valid_loss: 5.01710\n",
      "Validation loss decreased (5.042264 --> 5.017099).  Saving model ...\n",
      "[ 30/600] train_loss: 4.85440 valid_loss: 4.99316\n",
      "Validation loss decreased (5.017099 --> 4.993162).  Saving model ...\n",
      "[ 31/600] train_loss: 4.82914 valid_loss: 4.97033\n",
      "Validation loss decreased (4.993162 --> 4.970330).  Saving model ...\n",
      "[ 32/600] train_loss: 4.80506 valid_loss: 4.94845\n",
      "Validation loss decreased (4.970330 --> 4.948452).  Saving model ...\n",
      "[ 33/600] train_loss: 4.78210 valid_loss: 4.92754\n",
      "Validation loss decreased (4.948452 --> 4.927544).  Saving model ...\n",
      "[ 34/600] train_loss: 4.76007 valid_loss: 4.90763\n",
      "Validation loss decreased (4.927544 --> 4.907632).  Saving model ...\n",
      "[ 35/600] train_loss: 4.73909 valid_loss: 4.88859\n",
      "Validation loss decreased (4.907632 --> 4.888592).  Saving model ...\n",
      "[ 36/600] train_loss: 4.71894 valid_loss: 4.87026\n",
      "Validation loss decreased (4.888592 --> 4.870261).  Saving model ...\n",
      "[ 37/600] train_loss: 4.69969 valid_loss: 4.85283\n",
      "Validation loss decreased (4.870261 --> 4.852827).  Saving model ...\n",
      "[ 38/600] train_loss: 4.68113 valid_loss: 4.83614\n",
      "Validation loss decreased (4.852827 --> 4.836142).  Saving model ...\n",
      "[ 39/600] train_loss: 4.66342 valid_loss: 4.82020\n",
      "Validation loss decreased (4.836142 --> 4.820197).  Saving model ...\n",
      "[ 40/600] train_loss: 4.64634 valid_loss: 4.80487\n",
      "Validation loss decreased (4.820197 --> 4.804871).  Saving model ...\n",
      "[ 41/600] train_loss: 4.63003 valid_loss: 4.79026\n",
      "Validation loss decreased (4.804871 --> 4.790263).  Saving model ...\n",
      "[ 42/600] train_loss: 4.61437 valid_loss: 4.77617\n",
      "Validation loss decreased (4.790263 --> 4.776165).  Saving model ...\n",
      "[ 43/600] train_loss: 4.59925 valid_loss: 4.76281\n",
      "Validation loss decreased (4.776165 --> 4.762809).  Saving model ...\n",
      "[ 44/600] train_loss: 4.58468 valid_loss: 4.74992\n",
      "Validation loss decreased (4.762809 --> 4.749919).  Saving model ...\n",
      "[ 45/600] train_loss: 4.57073 valid_loss: 4.73763\n",
      "Validation loss decreased (4.749919 --> 4.737630).  Saving model ...\n",
      "[ 46/600] train_loss: 4.55728 valid_loss: 4.72571\n",
      "Validation loss decreased (4.737630 --> 4.725713).  Saving model ...\n",
      "[ 47/600] train_loss: 4.54419 valid_loss: 4.71436\n",
      "Validation loss decreased (4.725713 --> 4.714360).  Saving model ...\n",
      "[ 48/600] train_loss: 4.53177 valid_loss: 4.70343\n",
      "Validation loss decreased (4.714360 --> 4.703435).  Saving model ...\n",
      "[ 49/600] train_loss: 4.51971 valid_loss: 4.69297\n",
      "Validation loss decreased (4.703435 --> 4.692971).  Saving model ...\n",
      "[ 50/600] train_loss: 4.50807 valid_loss: 4.68292\n",
      "Validation loss decreased (4.692971 --> 4.682917).  Saving model ...\n",
      "[ 51/600] train_loss: 4.49677 valid_loss: 4.67317\n",
      "Validation loss decreased (4.682917 --> 4.673173).  Saving model ...\n",
      "[ 52/600] train_loss: 4.48581 valid_loss: 4.66389\n",
      "Validation loss decreased (4.673173 --> 4.663889).  Saving model ...\n",
      "[ 53/600] train_loss: 4.47528 valid_loss: 4.65491\n",
      "Validation loss decreased (4.663889 --> 4.654911).  Saving model ...\n",
      "[ 54/600] train_loss: 4.46513 valid_loss: 4.64627\n",
      "Validation loss decreased (4.654911 --> 4.646272).  Saving model ...\n",
      "[ 55/600] train_loss: 4.45520 valid_loss: 4.63786\n",
      "Validation loss decreased (4.646272 --> 4.637859).  Saving model ...\n",
      "[ 56/600] train_loss: 4.44566 valid_loss: 4.62993\n",
      "Validation loss decreased (4.637859 --> 4.629929).  Saving model ...\n",
      "[ 57/600] train_loss: 4.43637 valid_loss: 4.62209\n",
      "Validation loss decreased (4.629929 --> 4.622087).  Saving model ...\n",
      "[ 58/600] train_loss: 4.42743 valid_loss: 4.61467\n",
      "Validation loss decreased (4.622087 --> 4.614671).  Saving model ...\n",
      "[ 59/600] train_loss: 4.41866 valid_loss: 4.60743\n",
      "Validation loss decreased (4.614671 --> 4.607433).  Saving model ...\n",
      "[ 60/600] train_loss: 4.41023 valid_loss: 4.60046\n",
      "Validation loss decreased (4.607433 --> 4.600463).  Saving model ...\n",
      "[ 61/600] train_loss: 4.40196 valid_loss: 4.59364\n",
      "Validation loss decreased (4.600463 --> 4.593642).  Saving model ...\n",
      "[ 62/600] train_loss: 4.39392 valid_loss: 4.58704\n",
      "Validation loss decreased (4.593642 --> 4.587040).  Saving model ...\n",
      "[ 63/600] train_loss: 4.38618 valid_loss: 4.58063\n",
      "Validation loss decreased (4.587040 --> 4.580635).  Saving model ...\n",
      "[ 64/600] train_loss: 4.37853 valid_loss: 4.57449\n",
      "Validation loss decreased (4.580635 --> 4.574488).  Saving model ...\n",
      "[ 65/600] train_loss: 4.37123 valid_loss: 4.56846\n",
      "Validation loss decreased (4.574488 --> 4.568459).  Saving model ...\n",
      "[ 66/600] train_loss: 4.36400 valid_loss: 4.56258\n",
      "Validation loss decreased (4.568459 --> 4.562584).  Saving model ...\n",
      "[ 67/600] train_loss: 4.35698 valid_loss: 4.55689\n",
      "Validation loss decreased (4.562584 --> 4.556893).  Saving model ...\n",
      "[ 68/600] train_loss: 4.35016 valid_loss: 4.55137\n",
      "Validation loss decreased (4.556893 --> 4.551367).  Saving model ...\n",
      "[ 69/600] train_loss: 4.34348 valid_loss: 4.54605\n",
      "Validation loss decreased (4.551367 --> 4.546047).  Saving model ...\n",
      "[ 70/600] train_loss: 4.33688 valid_loss: 4.54075\n",
      "Validation loss decreased (4.546047 --> 4.540750).  Saving model ...\n",
      "[ 71/600] train_loss: 4.33064 valid_loss: 4.53572\n",
      "Validation loss decreased (4.540750 --> 4.535717).  Saving model ...\n",
      "[ 72/600] train_loss: 4.32447 valid_loss: 4.53079\n",
      "Validation loss decreased (4.535717 --> 4.530786).  Saving model ...\n",
      "[ 73/600] train_loss: 4.31839 valid_loss: 4.52577\n",
      "Validation loss decreased (4.530786 --> 4.525768).  Saving model ...\n",
      "[ 74/600] train_loss: 4.31253 valid_loss: 4.52100\n",
      "Validation loss decreased (4.525768 --> 4.521003).  Saving model ...\n",
      "[ 75/600] train_loss: 4.30663 valid_loss: 4.51645\n",
      "Validation loss decreased (4.521003 --> 4.516445).  Saving model ...\n",
      "[ 76/600] train_loss: 4.30103 valid_loss: 4.51190\n",
      "Validation loss decreased (4.516445 --> 4.511896).  Saving model ...\n",
      "[ 77/600] train_loss: 4.29555 valid_loss: 4.50751\n",
      "Validation loss decreased (4.511896 --> 4.507513).  Saving model ...\n",
      "[ 78/600] train_loss: 4.29008 valid_loss: 4.50322\n",
      "Validation loss decreased (4.507513 --> 4.503220).  Saving model ...\n",
      "[ 79/600] train_loss: 4.28481 valid_loss: 4.49896\n",
      "Validation loss decreased (4.503220 --> 4.498958).  Saving model ...\n",
      "[ 80/600] train_loss: 4.27964 valid_loss: 4.49488\n",
      "Validation loss decreased (4.498958 --> 4.494877).  Saving model ...\n",
      "[ 81/600] train_loss: 4.27456 valid_loss: 4.49089\n",
      "Validation loss decreased (4.494877 --> 4.490891).  Saving model ...\n",
      "[ 82/600] train_loss: 4.26955 valid_loss: 4.48684\n",
      "Validation loss decreased (4.490891 --> 4.486841).  Saving model ...\n",
      "[ 83/600] train_loss: 4.26476 valid_loss: 4.48293\n",
      "Validation loss decreased (4.486841 --> 4.482926).  Saving model ...\n",
      "[ 84/600] train_loss: 4.25987 valid_loss: 4.47911\n",
      "Validation loss decreased (4.482926 --> 4.479111).  Saving model ...\n",
      "[ 85/600] train_loss: 4.25527 valid_loss: 4.47540\n",
      "Validation loss decreased (4.479111 --> 4.475396).  Saving model ...\n",
      "[ 86/600] train_loss: 4.25063 valid_loss: 4.47171\n",
      "Validation loss decreased (4.475396 --> 4.471706).  Saving model ...\n",
      "[ 87/600] train_loss: 4.24618 valid_loss: 4.46820\n",
      "Validation loss decreased (4.471706 --> 4.468202).  Saving model ...\n",
      "[ 88/600] train_loss: 4.24166 valid_loss: 4.46460\n",
      "Validation loss decreased (4.468202 --> 4.464596).  Saving model ...\n",
      "[ 89/600] train_loss: 4.23736 valid_loss: 4.46107\n",
      "Validation loss decreased (4.464596 --> 4.461073).  Saving model ...\n",
      "[ 90/600] train_loss: 4.23301 valid_loss: 4.45773\n",
      "Validation loss decreased (4.461073 --> 4.457727).  Saving model ...\n",
      "[ 91/600] train_loss: 4.22890 valid_loss: 4.45441\n",
      "Validation loss decreased (4.457727 --> 4.454413).  Saving model ...\n",
      "[ 92/600] train_loss: 4.22475 valid_loss: 4.45116\n",
      "Validation loss decreased (4.454413 --> 4.451156).  Saving model ...\n",
      "[ 93/600] train_loss: 4.22071 valid_loss: 4.44790\n",
      "Validation loss decreased (4.451156 --> 4.447903).  Saving model ...\n",
      "[ 94/600] train_loss: 4.21681 valid_loss: 4.44471\n",
      "Validation loss decreased (4.447903 --> 4.444711).  Saving model ...\n",
      "[ 95/600] train_loss: 4.21285 valid_loss: 4.44165\n",
      "Validation loss decreased (4.444711 --> 4.441650).  Saving model ...\n",
      "[ 96/600] train_loss: 4.20904 valid_loss: 4.43852\n",
      "Validation loss decreased (4.441650 --> 4.438522).  Saving model ...\n",
      "[ 97/600] train_loss: 4.20516 valid_loss: 4.43554\n",
      "Validation loss decreased (4.438522 --> 4.435537).  Saving model ...\n",
      "[ 98/600] train_loss: 4.20151 valid_loss: 4.43254\n",
      "Validation loss decreased (4.435537 --> 4.432539).  Saving model ...\n",
      "[ 99/600] train_loss: 4.19782 valid_loss: 4.42962\n",
      "Validation loss decreased (4.432539 --> 4.429621).  Saving model ...\n",
      "[100/600] train_loss: 4.19425 valid_loss: 4.42676\n",
      "Validation loss decreased (4.429621 --> 4.426758).  Saving model ...\n",
      "[101/600] train_loss: 4.19071 valid_loss: 4.42390\n",
      "Validation loss decreased (4.426758 --> 4.423904).  Saving model ...\n",
      "[102/600] train_loss: 4.18725 valid_loss: 4.42114\n",
      "Validation loss decreased (4.423904 --> 4.421144).  Saving model ...\n",
      "[103/600] train_loss: 4.18382 valid_loss: 4.41844\n",
      "Validation loss decreased (4.421144 --> 4.418441).  Saving model ...\n",
      "[104/600] train_loss: 4.18043 valid_loss: 4.41578\n",
      "Validation loss decreased (4.418441 --> 4.415778).  Saving model ...\n",
      "[105/600] train_loss: 4.17712 valid_loss: 4.41314\n",
      "Validation loss decreased (4.415778 --> 4.413141).  Saving model ...\n",
      "[106/600] train_loss: 4.17383 valid_loss: 4.41050\n",
      "Validation loss decreased (4.413141 --> 4.410504).  Saving model ...\n",
      "[107/600] train_loss: 4.17065 valid_loss: 4.40788\n",
      "Validation loss decreased (4.410504 --> 4.407880).  Saving model ...\n",
      "[108/600] train_loss: 4.16752 valid_loss: 4.40549\n",
      "Validation loss decreased (4.407880 --> 4.405488).  Saving model ...\n",
      "[109/600] train_loss: 4.16430 valid_loss: 4.40285\n",
      "Validation loss decreased (4.405488 --> 4.402851).  Saving model ...\n",
      "[110/600] train_loss: 4.16119 valid_loss: 4.40042\n",
      "Validation loss decreased (4.402851 --> 4.400425).  Saving model ...\n",
      "[111/600] train_loss: 4.15817 valid_loss: 4.39787\n",
      "Validation loss decreased (4.400425 --> 4.397866).  Saving model ...\n",
      "[112/600] train_loss: 4.15519 valid_loss: 4.39556\n",
      "Validation loss decreased (4.397866 --> 4.395563).  Saving model ...\n",
      "[113/600] train_loss: 4.15222 valid_loss: 4.39314\n",
      "Validation loss decreased (4.395563 --> 4.393141).  Saving model ...\n",
      "[114/600] train_loss: 4.14931 valid_loss: 4.39077\n",
      "Validation loss decreased (4.393141 --> 4.390771).  Saving model ...\n",
      "[115/600] train_loss: 4.14644 valid_loss: 4.38854\n",
      "Validation loss decreased (4.390771 --> 4.388540).  Saving model ...\n",
      "[116/600] train_loss: 4.14361 valid_loss: 4.38628\n",
      "Validation loss decreased (4.388540 --> 4.386281).  Saving model ...\n",
      "[117/600] train_loss: 4.14077 valid_loss: 4.38396\n",
      "Validation loss decreased (4.386281 --> 4.383957).  Saving model ...\n",
      "[118/600] train_loss: 4.13808 valid_loss: 4.38185\n",
      "Validation loss decreased (4.383957 --> 4.381846).  Saving model ...\n",
      "[119/600] train_loss: 4.13528 valid_loss: 4.37965\n",
      "Validation loss decreased (4.381846 --> 4.379654).  Saving model ...\n",
      "[120/600] train_loss: 4.13261 valid_loss: 4.37737\n",
      "Validation loss decreased (4.379654 --> 4.377369).  Saving model ...\n",
      "[121/600] train_loss: 4.13000 valid_loss: 4.37533\n",
      "Validation loss decreased (4.377369 --> 4.375327).  Saving model ...\n",
      "[122/600] train_loss: 4.12738 valid_loss: 4.37323\n",
      "Validation loss decreased (4.375327 --> 4.373228).  Saving model ...\n",
      "[123/600] train_loss: 4.12476 valid_loss: 4.37113\n",
      "Validation loss decreased (4.373228 --> 4.371133).  Saving model ...\n",
      "[124/600] train_loss: 4.12224 valid_loss: 4.36908\n",
      "Validation loss decreased (4.371133 --> 4.369082).  Saving model ...\n",
      "[125/600] train_loss: 4.11968 valid_loss: 4.36712\n",
      "Validation loss decreased (4.369082 --> 4.367124).  Saving model ...\n",
      "[126/600] train_loss: 4.11724 valid_loss: 4.36508\n",
      "Validation loss decreased (4.367124 --> 4.365084).  Saving model ...\n",
      "[127/600] train_loss: 4.11480 valid_loss: 4.36311\n",
      "Validation loss decreased (4.365084 --> 4.363114).  Saving model ...\n",
      "[128/600] train_loss: 4.11231 valid_loss: 4.36110\n",
      "Validation loss decreased (4.363114 --> 4.361096).  Saving model ...\n",
      "[129/600] train_loss: 4.10993 valid_loss: 4.35921\n",
      "Validation loss decreased (4.361096 --> 4.359212).  Saving model ...\n",
      "[130/600] train_loss: 4.10756 valid_loss: 4.35737\n",
      "Validation loss decreased (4.359212 --> 4.357366).  Saving model ...\n",
      "[131/600] train_loss: 4.10522 valid_loss: 4.35545\n",
      "Validation loss decreased (4.357366 --> 4.355448).  Saving model ...\n",
      "[132/600] train_loss: 4.10290 valid_loss: 4.35359\n",
      "Validation loss decreased (4.355448 --> 4.353586).  Saving model ...\n",
      "[133/600] train_loss: 4.10056 valid_loss: 4.35172\n",
      "Validation loss decreased (4.353586 --> 4.351725).  Saving model ...\n",
      "[134/600] train_loss: 4.09831 valid_loss: 4.34991\n",
      "Validation loss decreased (4.351725 --> 4.349908).  Saving model ...\n",
      "[135/600] train_loss: 4.09611 valid_loss: 4.34812\n",
      "Validation loss decreased (4.349908 --> 4.348125).  Saving model ...\n",
      "[136/600] train_loss: 4.09386 valid_loss: 4.34642\n",
      "Validation loss decreased (4.348125 --> 4.346417).  Saving model ...\n",
      "[137/600] train_loss: 4.09174 valid_loss: 4.34462\n",
      "Validation loss decreased (4.346417 --> 4.344615).  Saving model ...\n",
      "[138/600] train_loss: 4.08950 valid_loss: 4.34285\n",
      "Validation loss decreased (4.344615 --> 4.342848).  Saving model ...\n",
      "[139/600] train_loss: 4.08737 valid_loss: 4.34115\n",
      "Validation loss decreased (4.342848 --> 4.341155).  Saving model ...\n",
      "[140/600] train_loss: 4.08526 valid_loss: 4.33947\n",
      "Validation loss decreased (4.341155 --> 4.339471).  Saving model ...\n",
      "[141/600] train_loss: 4.08320 valid_loss: 4.33784\n",
      "Validation loss decreased (4.339471 --> 4.337843).  Saving model ...\n",
      "[142/600] train_loss: 4.08111 valid_loss: 4.33612\n",
      "Validation loss decreased (4.337843 --> 4.336119).  Saving model ...\n",
      "[143/600] train_loss: 4.07906 valid_loss: 4.33446\n",
      "Validation loss decreased (4.336119 --> 4.334461).  Saving model ...\n",
      "[144/600] train_loss: 4.07705 valid_loss: 4.33289\n",
      "Validation loss decreased (4.334461 --> 4.332889).  Saving model ...\n",
      "[145/600] train_loss: 4.07507 valid_loss: 4.33131\n",
      "Validation loss decreased (4.332889 --> 4.331311).  Saving model ...\n",
      "[146/600] train_loss: 4.07310 valid_loss: 4.32962\n",
      "Validation loss decreased (4.331311 --> 4.329617).  Saving model ...\n",
      "[147/600] train_loss: 4.07115 valid_loss: 4.32814\n",
      "Validation loss decreased (4.329617 --> 4.328140).  Saving model ...\n",
      "[148/600] train_loss: 4.06923 valid_loss: 4.32660\n",
      "Validation loss decreased (4.328140 --> 4.326598).  Saving model ...\n",
      "[149/600] train_loss: 4.06728 valid_loss: 4.32502\n",
      "Validation loss decreased (4.326598 --> 4.325023).  Saving model ...\n",
      "[150/600] train_loss: 4.06536 valid_loss: 4.32344\n",
      "Validation loss decreased (4.325023 --> 4.323439).  Saving model ...\n",
      "[151/600] train_loss: 4.06350 valid_loss: 4.32197\n",
      "Validation loss decreased (4.323439 --> 4.321969).  Saving model ...\n",
      "[152/600] train_loss: 4.06162 valid_loss: 4.32049\n",
      "Validation loss decreased (4.321969 --> 4.320487).  Saving model ...\n",
      "[153/600] train_loss: 4.05975 valid_loss: 4.31898\n",
      "Validation loss decreased (4.320487 --> 4.318979).  Saving model ...\n",
      "[154/600] train_loss: 4.05790 valid_loss: 4.31760\n",
      "Validation loss decreased (4.318979 --> 4.317598).  Saving model ...\n",
      "[155/600] train_loss: 4.05612 valid_loss: 4.31619\n",
      "Validation loss decreased (4.317598 --> 4.316188).  Saving model ...\n",
      "[156/600] train_loss: 4.05437 valid_loss: 4.31475\n",
      "Validation loss decreased (4.316188 --> 4.314747).  Saving model ...\n",
      "[157/600] train_loss: 4.05260 valid_loss: 4.31337\n",
      "Validation loss decreased (4.314747 --> 4.313372).  Saving model ...\n",
      "[158/600] train_loss: 4.05082 valid_loss: 4.31187\n",
      "Validation loss decreased (4.313372 --> 4.311868).  Saving model ...\n",
      "[159/600] train_loss: 4.04909 valid_loss: 4.31060\n",
      "Validation loss decreased (4.311868 --> 4.310598).  Saving model ...\n",
      "[160/600] train_loss: 4.04732 valid_loss: 4.30910\n",
      "Validation loss decreased (4.310598 --> 4.309097).  Saving model ...\n",
      "[161/600] train_loss: 4.04565 valid_loss: 4.30772\n",
      "Validation loss decreased (4.309097 --> 4.307721).  Saving model ...\n",
      "[162/600] train_loss: 4.04393 valid_loss: 4.30639\n",
      "Validation loss decreased (4.307721 --> 4.306393).  Saving model ...\n",
      "[163/600] train_loss: 4.04229 valid_loss: 4.30514\n",
      "Validation loss decreased (4.306393 --> 4.305142).  Saving model ...\n",
      "[164/600] train_loss: 4.04060 valid_loss: 4.30382\n",
      "Validation loss decreased (4.305142 --> 4.303824).  Saving model ...\n",
      "[165/600] train_loss: 4.03897 valid_loss: 4.30247\n",
      "Validation loss decreased (4.303824 --> 4.302471).  Saving model ...\n",
      "[166/600] train_loss: 4.03738 valid_loss: 4.30119\n",
      "Validation loss decreased (4.302471 --> 4.301192).  Saving model ...\n",
      "[167/600] train_loss: 4.03575 valid_loss: 4.29990\n",
      "Validation loss decreased (4.301192 --> 4.299903).  Saving model ...\n",
      "[168/600] train_loss: 4.03414 valid_loss: 4.29862\n",
      "Validation loss decreased (4.299903 --> 4.298618).  Saving model ...\n",
      "[169/600] train_loss: 4.03249 valid_loss: 4.29747\n",
      "Validation loss decreased (4.298618 --> 4.297473).  Saving model ...\n",
      "[170/600] train_loss: 4.03095 valid_loss: 4.29611\n",
      "Validation loss decreased (4.297473 --> 4.296114).  Saving model ...\n",
      "[171/600] train_loss: 4.02940 valid_loss: 4.29489\n",
      "Validation loss decreased (4.296114 --> 4.294886).  Saving model ...\n",
      "[172/600] train_loss: 4.02794 valid_loss: 4.29361\n",
      "Validation loss decreased (4.294886 --> 4.293606).  Saving model ...\n",
      "[173/600] train_loss: 4.02636 valid_loss: 4.29236\n",
      "Validation loss decreased (4.293606 --> 4.292363).  Saving model ...\n",
      "[174/600] train_loss: 4.02481 valid_loss: 4.29126\n",
      "Validation loss decreased (4.292363 --> 4.291257).  Saving model ...\n",
      "[175/600] train_loss: 4.02330 valid_loss: 4.29009\n",
      "Validation loss decreased (4.291257 --> 4.290088).  Saving model ...\n",
      "[176/600] train_loss: 4.02186 valid_loss: 4.28879\n",
      "Validation loss decreased (4.290088 --> 4.288795).  Saving model ...\n",
      "[177/600] train_loss: 4.02039 valid_loss: 4.28763\n",
      "Validation loss decreased (4.288795 --> 4.287630).  Saving model ...\n",
      "[178/600] train_loss: 4.01890 valid_loss: 4.28649\n",
      "Validation loss decreased (4.287630 --> 4.286487).  Saving model ...\n",
      "[179/600] train_loss: 4.01747 valid_loss: 4.28537\n",
      "Validation loss decreased (4.286487 --> 4.285369).  Saving model ...\n",
      "[180/600] train_loss: 4.01602 valid_loss: 4.28423\n",
      "Validation loss decreased (4.285369 --> 4.284229).  Saving model ...\n",
      "[181/600] train_loss: 4.01463 valid_loss: 4.28306\n",
      "Validation loss decreased (4.284229 --> 4.283057).  Saving model ...\n",
      "[182/600] train_loss: 4.01317 valid_loss: 4.28188\n",
      "Validation loss decreased (4.283057 --> 4.281877).  Saving model ...\n",
      "[183/600] train_loss: 4.01176 valid_loss: 4.28074\n",
      "Validation loss decreased (4.281877 --> 4.280744).  Saving model ...\n",
      "[184/600] train_loss: 4.01039 valid_loss: 4.27974\n",
      "Validation loss decreased (4.280744 --> 4.279744).  Saving model ...\n",
      "[185/600] train_loss: 4.00908 valid_loss: 4.27860\n",
      "Validation loss decreased (4.279744 --> 4.278596).  Saving model ...\n",
      "[186/600] train_loss: 4.00760 valid_loss: 4.27753\n",
      "Validation loss decreased (4.278596 --> 4.277535).  Saving model ...\n",
      "[187/600] train_loss: 4.00624 valid_loss: 4.27649\n",
      "Validation loss decreased (4.277535 --> 4.276492).  Saving model ...\n",
      "[188/600] train_loss: 4.00489 valid_loss: 4.27537\n",
      "Validation loss decreased (4.276492 --> 4.275367).  Saving model ...\n",
      "[189/600] train_loss: 4.00356 valid_loss: 4.27436\n",
      "Validation loss decreased (4.275367 --> 4.274358).  Saving model ...\n",
      "[190/600] train_loss: 4.00227 valid_loss: 4.27329\n",
      "Validation loss decreased (4.274358 --> 4.273286).  Saving model ...\n",
      "[191/600] train_loss: 4.00099 valid_loss: 4.27226\n",
      "Validation loss decreased (4.273286 --> 4.272260).  Saving model ...\n",
      "[192/600] train_loss: 3.99961 valid_loss: 4.27120\n",
      "Validation loss decreased (4.272260 --> 4.271196).  Saving model ...\n",
      "[193/600] train_loss: 3.99835 valid_loss: 4.27020\n",
      "Validation loss decreased (4.271196 --> 4.270200).  Saving model ...\n",
      "[194/600] train_loss: 3.99704 valid_loss: 4.26917\n",
      "Validation loss decreased (4.270200 --> 4.269175).  Saving model ...\n",
      "[195/600] train_loss: 3.99575 valid_loss: 4.26813\n",
      "Validation loss decreased (4.269175 --> 4.268128).  Saving model ...\n",
      "[196/600] train_loss: 3.99451 valid_loss: 4.26710\n",
      "Validation loss decreased (4.268128 --> 4.267097).  Saving model ...\n",
      "[197/600] train_loss: 3.99326 valid_loss: 4.26614\n",
      "Validation loss decreased (4.267097 --> 4.266138).  Saving model ...\n",
      "[198/600] train_loss: 3.99202 valid_loss: 4.26520\n",
      "Validation loss decreased (4.266138 --> 4.265204).  Saving model ...\n",
      "[199/600] train_loss: 3.99076 valid_loss: 4.26425\n",
      "Validation loss decreased (4.265204 --> 4.264247).  Saving model ...\n",
      "[200/600] train_loss: 3.98959 valid_loss: 4.26326\n",
      "Validation loss decreased (4.264247 --> 4.263263).  Saving model ...\n",
      "[201/600] train_loss: 3.98834 valid_loss: 4.26231\n",
      "Validation loss decreased (4.263263 --> 4.262311).  Saving model ...\n",
      "[202/600] train_loss: 3.98710 valid_loss: 4.26134\n",
      "Validation loss decreased (4.262311 --> 4.261339).  Saving model ...\n",
      "[203/600] train_loss: 3.98587 valid_loss: 4.26045\n",
      "Validation loss decreased (4.261339 --> 4.260450).  Saving model ...\n",
      "[204/600] train_loss: 3.98471 valid_loss: 4.25953\n",
      "Validation loss decreased (4.260450 --> 4.259529).  Saving model ...\n",
      "[205/600] train_loss: 3.98358 valid_loss: 4.25858\n",
      "Validation loss decreased (4.259529 --> 4.258576).  Saving model ...\n",
      "[206/600] train_loss: 3.98234 valid_loss: 4.25764\n",
      "Validation loss decreased (4.258576 --> 4.257643).  Saving model ...\n",
      "[207/600] train_loss: 3.98112 valid_loss: 4.25666\n",
      "Validation loss decreased (4.257643 --> 4.256662).  Saving model ...\n",
      "[208/600] train_loss: 3.98009 valid_loss: 4.25581\n",
      "Validation loss decreased (4.256662 --> 4.255811).  Saving model ...\n",
      "[209/600] train_loss: 3.97889 valid_loss: 4.25490\n",
      "Validation loss decreased (4.255811 --> 4.254898).  Saving model ...\n",
      "[210/600] train_loss: 3.97776 valid_loss: 4.25400\n",
      "Validation loss decreased (4.254898 --> 4.253997).  Saving model ...\n",
      "[211/600] train_loss: 3.97663 valid_loss: 4.25315\n",
      "Validation loss decreased (4.253997 --> 4.253150).  Saving model ...\n",
      "[212/600] train_loss: 3.97551 valid_loss: 4.25228\n",
      "Validation loss decreased (4.253150 --> 4.252285).  Saving model ...\n",
      "[213/600] train_loss: 3.97439 valid_loss: 4.25132\n",
      "Validation loss decreased (4.252285 --> 4.251323).  Saving model ...\n",
      "[214/600] train_loss: 3.97330 valid_loss: 4.25047\n",
      "Validation loss decreased (4.251323 --> 4.250472).  Saving model ...\n",
      "[215/600] train_loss: 3.97219 valid_loss: 4.24970\n",
      "Validation loss decreased (4.250472 --> 4.249700).  Saving model ...\n",
      "[216/600] train_loss: 3.97110 valid_loss: 4.24871\n",
      "Validation loss decreased (4.249700 --> 4.248710).  Saving model ...\n",
      "[217/600] train_loss: 3.96995 valid_loss: 4.24791\n",
      "Validation loss decreased (4.248710 --> 4.247915).  Saving model ...\n",
      "[218/600] train_loss: 3.96890 valid_loss: 4.24711\n",
      "Validation loss decreased (4.247915 --> 4.247114).  Saving model ...\n",
      "[219/600] train_loss: 3.96784 valid_loss: 4.24631\n",
      "Validation loss decreased (4.247114 --> 4.246314).  Saving model ...\n",
      "[220/600] train_loss: 3.96682 valid_loss: 4.24538\n",
      "Validation loss decreased (4.246314 --> 4.245376).  Saving model ...\n",
      "[221/600] train_loss: 3.96569 valid_loss: 4.24467\n",
      "Validation loss decreased (4.245376 --> 4.244671).  Saving model ...\n",
      "[222/600] train_loss: 3.96464 valid_loss: 4.24375\n",
      "Validation loss decreased (4.244671 --> 4.243747).  Saving model ...\n",
      "[223/600] train_loss: 3.96363 valid_loss: 4.24299\n",
      "Validation loss decreased (4.243747 --> 4.242989).  Saving model ...\n",
      "[224/600] train_loss: 3.96260 valid_loss: 4.24220\n",
      "Validation loss decreased (4.242989 --> 4.242199).  Saving model ...\n",
      "[225/600] train_loss: 3.96152 valid_loss: 4.24137\n",
      "Validation loss decreased (4.242199 --> 4.241371).  Saving model ...\n",
      "[226/600] train_loss: 3.96054 valid_loss: 4.24055\n",
      "Validation loss decreased (4.241371 --> 4.240549).  Saving model ...\n",
      "[227/600] train_loss: 3.95954 valid_loss: 4.23979\n",
      "Validation loss decreased (4.240549 --> 4.239794).  Saving model ...\n",
      "[228/600] train_loss: 3.95850 valid_loss: 4.23908\n",
      "Validation loss decreased (4.239794 --> 4.239079).  Saving model ...\n",
      "[229/600] train_loss: 3.95756 valid_loss: 4.23829\n",
      "Validation loss decreased (4.239079 --> 4.238287).  Saving model ...\n",
      "[230/600] train_loss: 3.95656 valid_loss: 4.23745\n",
      "Validation loss decreased (4.238287 --> 4.237454).  Saving model ...\n",
      "[231/600] train_loss: 3.95554 valid_loss: 4.23671\n",
      "Validation loss decreased (4.237454 --> 4.236714).  Saving model ...\n",
      "[232/600] train_loss: 3.95452 valid_loss: 4.23598\n",
      "Validation loss decreased (4.236714 --> 4.235976).  Saving model ...\n",
      "[233/600] train_loss: 3.95357 valid_loss: 4.23517\n",
      "Validation loss decreased (4.235976 --> 4.235169).  Saving model ...\n",
      "[234/600] train_loss: 3.95262 valid_loss: 4.23444\n",
      "Validation loss decreased (4.235169 --> 4.234444).  Saving model ...\n",
      "[235/600] train_loss: 3.95161 valid_loss: 4.23367\n",
      "Validation loss decreased (4.234444 --> 4.233670).  Saving model ...\n",
      "[236/600] train_loss: 3.95067 valid_loss: 4.23303\n",
      "Validation loss decreased (4.233670 --> 4.233026).  Saving model ...\n",
      "[237/600] train_loss: 3.94977 valid_loss: 4.23225\n",
      "Validation loss decreased (4.233026 --> 4.232247).  Saving model ...\n",
      "[238/600] train_loss: 3.94884 valid_loss: 4.23154\n",
      "Validation loss decreased (4.232247 --> 4.231541).  Saving model ...\n",
      "[239/600] train_loss: 3.94778 valid_loss: 4.23080\n",
      "Validation loss decreased (4.231541 --> 4.230805).  Saving model ...\n",
      "[240/600] train_loss: 3.94696 valid_loss: 4.23001\n",
      "Validation loss decreased (4.230805 --> 4.230005).  Saving model ...\n",
      "[241/600] train_loss: 3.94601 valid_loss: 4.22939\n",
      "Validation loss decreased (4.230005 --> 4.229395).  Saving model ...\n",
      "[242/600] train_loss: 3.94507 valid_loss: 4.22866\n",
      "Validation loss decreased (4.229395 --> 4.228660).  Saving model ...\n",
      "[243/600] train_loss: 3.94416 valid_loss: 4.22793\n",
      "Validation loss decreased (4.228660 --> 4.227934).  Saving model ...\n",
      "[244/600] train_loss: 3.94326 valid_loss: 4.22724\n",
      "Validation loss decreased (4.227934 --> 4.227242).  Saving model ...\n",
      "[245/600] train_loss: 3.94235 valid_loss: 4.22656\n",
      "Validation loss decreased (4.227242 --> 4.226557).  Saving model ...\n",
      "[246/600] train_loss: 3.94145 valid_loss: 4.22585\n",
      "Validation loss decreased (4.226557 --> 4.225854).  Saving model ...\n",
      "[247/600] train_loss: 3.94056 valid_loss: 4.22513\n",
      "Validation loss decreased (4.225854 --> 4.225129).  Saving model ...\n",
      "[248/600] train_loss: 3.93968 valid_loss: 4.22452\n",
      "Validation loss decreased (4.225129 --> 4.224519).  Saving model ...\n",
      "[249/600] train_loss: 3.93878 valid_loss: 4.22379\n",
      "Validation loss decreased (4.224519 --> 4.223791).  Saving model ...\n",
      "[250/600] train_loss: 3.93791 valid_loss: 4.22316\n",
      "Validation loss decreased (4.223791 --> 4.223158).  Saving model ...\n",
      "[251/600] train_loss: 3.93702 valid_loss: 4.22248\n",
      "Validation loss decreased (4.223158 --> 4.222483).  Saving model ...\n",
      "[252/600] train_loss: 3.93616 valid_loss: 4.22184\n",
      "Validation loss decreased (4.222483 --> 4.221839).  Saving model ...\n",
      "[253/600] train_loss: 3.93530 valid_loss: 4.22119\n",
      "Validation loss decreased (4.221839 --> 4.221186).  Saving model ...\n",
      "[254/600] train_loss: 3.93446 valid_loss: 4.22055\n",
      "Validation loss decreased (4.221186 --> 4.220548).  Saving model ...\n",
      "[255/600] train_loss: 3.93356 valid_loss: 4.21981\n",
      "Validation loss decreased (4.220548 --> 4.219808).  Saving model ...\n",
      "[256/600] train_loss: 3.93276 valid_loss: 4.21921\n",
      "Validation loss decreased (4.219808 --> 4.219208).  Saving model ...\n",
      "[257/600] train_loss: 3.93190 valid_loss: 4.21860\n",
      "Validation loss decreased (4.219208 --> 4.218597).  Saving model ...\n",
      "[258/600] train_loss: 3.93106 valid_loss: 4.21800\n",
      "Validation loss decreased (4.218597 --> 4.217997).  Saving model ...\n",
      "[259/600] train_loss: 3.93025 valid_loss: 4.21730\n",
      "Validation loss decreased (4.217997 --> 4.217296).  Saving model ...\n",
      "[260/600] train_loss: 3.92940 valid_loss: 4.21670\n",
      "Validation loss decreased (4.217296 --> 4.216699).  Saving model ...\n",
      "[261/600] train_loss: 3.92860 valid_loss: 4.21609\n",
      "Validation loss decreased (4.216699 --> 4.216090).  Saving model ...\n",
      "[262/600] train_loss: 3.92773 valid_loss: 4.21548\n",
      "Validation loss decreased (4.216090 --> 4.215484).  Saving model ...\n",
      "[263/600] train_loss: 3.92693 valid_loss: 4.21484\n",
      "Validation loss decreased (4.215484 --> 4.214840).  Saving model ...\n",
      "[264/600] train_loss: 3.92612 valid_loss: 4.21426\n",
      "Validation loss decreased (4.214840 --> 4.214263).  Saving model ...\n",
      "[265/600] train_loss: 3.92535 valid_loss: 4.21361\n",
      "Validation loss decreased (4.214263 --> 4.213612).  Saving model ...\n",
      "[266/600] train_loss: 3.92452 valid_loss: 4.21292\n",
      "Validation loss decreased (4.213612 --> 4.212917).  Saving model ...\n",
      "[267/600] train_loss: 3.92372 valid_loss: 4.21242\n",
      "Validation loss decreased (4.212917 --> 4.212423).  Saving model ...\n",
      "[268/600] train_loss: 3.92296 valid_loss: 4.21184\n",
      "Validation loss decreased (4.212423 --> 4.211843).  Saving model ...\n",
      "[269/600] train_loss: 3.92216 valid_loss: 4.21113\n",
      "Validation loss decreased (4.211843 --> 4.211130).  Saving model ...\n",
      "[270/600] train_loss: 3.92142 valid_loss: 4.21062\n",
      "Validation loss decreased (4.211130 --> 4.210623).  Saving model ...\n",
      "[271/600] train_loss: 3.92061 valid_loss: 4.21007\n",
      "Validation loss decreased (4.210623 --> 4.210066).  Saving model ...\n",
      "[272/600] train_loss: 3.91988 valid_loss: 4.20944\n",
      "Validation loss decreased (4.210066 --> 4.209441).  Saving model ...\n",
      "[273/600] train_loss: 3.91902 valid_loss: 4.20900\n",
      "Validation loss decreased (4.209441 --> 4.209003).  Saving model ...\n",
      "[274/600] train_loss: 3.91825 valid_loss: 4.20830\n",
      "Validation loss decreased (4.209003 --> 4.208302).  Saving model ...\n",
      "[275/600] train_loss: 3.91752 valid_loss: 4.20773\n",
      "Validation loss decreased (4.208302 --> 4.207732).  Saving model ...\n",
      "[276/600] train_loss: 3.91679 valid_loss: 4.20729\n",
      "Validation loss decreased (4.207732 --> 4.207286).  Saving model ...\n",
      "[277/600] train_loss: 3.91607 valid_loss: 4.20666\n",
      "Validation loss decreased (4.207286 --> 4.206662).  Saving model ...\n",
      "[278/600] train_loss: 3.91531 valid_loss: 4.20613\n",
      "Validation loss decreased (4.206662 --> 4.206129).  Saving model ...\n",
      "[279/600] train_loss: 3.91453 valid_loss: 4.20552\n",
      "Validation loss decreased (4.206129 --> 4.205517).  Saving model ...\n",
      "[280/600] train_loss: 3.91375 valid_loss: 4.20493\n",
      "Validation loss decreased (4.205517 --> 4.204931).  Saving model ...\n",
      "[281/600] train_loss: 3.91305 valid_loss: 4.20447\n",
      "Validation loss decreased (4.204931 --> 4.204475).  Saving model ...\n",
      "[282/600] train_loss: 3.91235 valid_loss: 4.20381\n",
      "Validation loss decreased (4.204475 --> 4.203813).  Saving model ...\n",
      "[283/600] train_loss: 3.91159 valid_loss: 4.20331\n",
      "Validation loss decreased (4.203813 --> 4.203315).  Saving model ...\n",
      "[284/600] train_loss: 3.91082 valid_loss: 4.20273\n",
      "Validation loss decreased (4.203315 --> 4.202726).  Saving model ...\n",
      "[285/600] train_loss: 3.91014 valid_loss: 4.20222\n",
      "Validation loss decreased (4.202726 --> 4.202222).  Saving model ...\n",
      "[286/600] train_loss: 3.90939 valid_loss: 4.20174\n",
      "Validation loss decreased (4.202222 --> 4.201742).  Saving model ...\n",
      "[287/600] train_loss: 3.90868 valid_loss: 4.20117\n",
      "Validation loss decreased (4.201742 --> 4.201173).  Saving model ...\n",
      "[288/600] train_loss: 3.90801 valid_loss: 4.20066\n",
      "Validation loss decreased (4.201173 --> 4.200655).  Saving model ...\n",
      "[289/600] train_loss: 3.90732 valid_loss: 4.20020\n",
      "Validation loss decreased (4.200655 --> 4.200200).  Saving model ...\n",
      "[290/600] train_loss: 3.90657 valid_loss: 4.19968\n",
      "Validation loss decreased (4.200200 --> 4.199676).  Saving model ...\n",
      "[291/600] train_loss: 3.90584 valid_loss: 4.19914\n",
      "Validation loss decreased (4.199676 --> 4.199143).  Saving model ...\n",
      "[292/600] train_loss: 3.90515 valid_loss: 4.19861\n",
      "Validation loss decreased (4.199143 --> 4.198614).  Saving model ...\n",
      "[293/600] train_loss: 3.90449 valid_loss: 4.19807\n",
      "Validation loss decreased (4.198614 --> 4.198071).  Saving model ...\n",
      "[294/600] train_loss: 3.90393 valid_loss: 4.19754\n",
      "Validation loss decreased (4.198071 --> 4.197543).  Saving model ...\n",
      "[295/600] train_loss: 3.90314 valid_loss: 4.19700\n",
      "Validation loss decreased (4.197543 --> 4.196999).  Saving model ...\n",
      "[296/600] train_loss: 3.90246 valid_loss: 4.19658\n",
      "Validation loss decreased (4.196999 --> 4.196577).  Saving model ...\n",
      "[297/600] train_loss: 3.90177 valid_loss: 4.19610\n",
      "Validation loss decreased (4.196577 --> 4.196096).  Saving model ...\n",
      "[298/600] train_loss: 3.90114 valid_loss: 4.19566\n",
      "Validation loss decreased (4.196096 --> 4.195660).  Saving model ...\n",
      "[299/600] train_loss: 3.90043 valid_loss: 4.19509\n",
      "Validation loss decreased (4.195660 --> 4.195087).  Saving model ...\n",
      "[300/600] train_loss: 3.89974 valid_loss: 4.19457\n",
      "Validation loss decreased (4.195087 --> 4.194568).  Saving model ...\n",
      "[301/600] train_loss: 3.89914 valid_loss: 4.19408\n",
      "Validation loss decreased (4.194568 --> 4.194076).  Saving model ...\n",
      "[302/600] train_loss: 3.89840 valid_loss: 4.19360\n",
      "Validation loss decreased (4.194076 --> 4.193598).  Saving model ...\n",
      "[303/600] train_loss: 3.89775 valid_loss: 4.19314\n",
      "Validation loss decreased (4.193598 --> 4.193136).  Saving model ...\n",
      "[304/600] train_loss: 3.89712 valid_loss: 4.19262\n",
      "Validation loss decreased (4.193136 --> 4.192622).  Saving model ...\n",
      "[305/600] train_loss: 3.89639 valid_loss: 4.19215\n",
      "Validation loss decreased (4.192622 --> 4.192150).  Saving model ...\n",
      "[306/600] train_loss: 3.89577 valid_loss: 4.19171\n",
      "Validation loss decreased (4.192150 --> 4.191706).  Saving model ...\n",
      "[307/600] train_loss: 3.89512 valid_loss: 4.19128\n",
      "Validation loss decreased (4.191706 --> 4.191280).  Saving model ...\n",
      "[308/600] train_loss: 3.89452 valid_loss: 4.19085\n",
      "Validation loss decreased (4.191280 --> 4.190849).  Saving model ...\n",
      "[309/600] train_loss: 3.89390 valid_loss: 4.19038\n",
      "Validation loss decreased (4.190849 --> 4.190376).  Saving model ...\n",
      "[310/600] train_loss: 3.89321 valid_loss: 4.18989\n",
      "Validation loss decreased (4.190376 --> 4.189885).  Saving model ...\n",
      "[311/600] train_loss: 3.89257 valid_loss: 4.18940\n",
      "Validation loss decreased (4.189885 --> 4.189402).  Saving model ...\n",
      "[312/600] train_loss: 3.89199 valid_loss: 4.18897\n",
      "Validation loss decreased (4.189402 --> 4.188969).  Saving model ...\n",
      "[313/600] train_loss: 3.89134 valid_loss: 4.18837\n",
      "Validation loss decreased (4.188969 --> 4.188371).  Saving model ...\n",
      "[314/600] train_loss: 3.89073 valid_loss: 4.18805\n",
      "Validation loss decreased (4.188371 --> 4.188048).  Saving model ...\n",
      "[315/600] train_loss: 3.89010 valid_loss: 4.18762\n",
      "Validation loss decreased (4.188048 --> 4.187621).  Saving model ...\n",
      "[316/600] train_loss: 3.88946 valid_loss: 4.18711\n",
      "Validation loss decreased (4.187621 --> 4.187107).  Saving model ...\n",
      "[317/600] train_loss: 3.88888 valid_loss: 4.18659\n",
      "Validation loss decreased (4.187107 --> 4.186595).  Saving model ...\n",
      "[318/600] train_loss: 3.88822 valid_loss: 4.18620\n",
      "Validation loss decreased (4.186595 --> 4.186198).  Saving model ...\n",
      "[319/600] train_loss: 3.88761 valid_loss: 4.18574\n",
      "Validation loss decreased (4.186198 --> 4.185745).  Saving model ...\n",
      "[320/600] train_loss: 3.88707 valid_loss: 4.18536\n",
      "Validation loss decreased (4.185745 --> 4.185358).  Saving model ...\n",
      "[321/600] train_loss: 3.88644 valid_loss: 4.18485\n",
      "Validation loss decreased (4.185358 --> 4.184851).  Saving model ...\n",
      "[322/600] train_loss: 3.88589 valid_loss: 4.18444\n",
      "Validation loss decreased (4.184851 --> 4.184442).  Saving model ...\n",
      "[323/600] train_loss: 3.88526 valid_loss: 4.18409\n",
      "Validation loss decreased (4.184442 --> 4.184086).  Saving model ...\n",
      "[324/600] train_loss: 3.88470 valid_loss: 4.18363\n",
      "Validation loss decreased (4.184086 --> 4.183634).  Saving model ...\n",
      "[325/600] train_loss: 3.88412 valid_loss: 4.18324\n",
      "Validation loss decreased (4.183634 --> 4.183237).  Saving model ...\n",
      "[326/600] train_loss: 3.88346 valid_loss: 4.18283\n",
      "Validation loss decreased (4.183237 --> 4.182831).  Saving model ...\n",
      "[327/600] train_loss: 3.88289 valid_loss: 4.18240\n",
      "Validation loss decreased (4.182831 --> 4.182402).  Saving model ...\n",
      "[328/600] train_loss: 3.88232 valid_loss: 4.18192\n",
      "Validation loss decreased (4.182402 --> 4.181917).  Saving model ...\n",
      "[329/600] train_loss: 3.88178 valid_loss: 4.18147\n",
      "Validation loss decreased (4.181917 --> 4.181474).  Saving model ...\n",
      "[330/600] train_loss: 3.88118 valid_loss: 4.18108\n",
      "Validation loss decreased (4.181474 --> 4.181081).  Saving model ...\n",
      "[331/600] train_loss: 3.88061 valid_loss: 4.18066\n",
      "Validation loss decreased (4.181081 --> 4.180655).  Saving model ...\n",
      "[332/600] train_loss: 3.88003 valid_loss: 4.18025\n",
      "Validation loss decreased (4.180655 --> 4.180247).  Saving model ...\n",
      "[333/600] train_loss: 3.87946 valid_loss: 4.17984\n",
      "Validation loss decreased (4.180247 --> 4.179844).  Saving model ...\n",
      "[334/600] train_loss: 3.87882 valid_loss: 4.17940\n",
      "Validation loss decreased (4.179844 --> 4.179403).  Saving model ...\n",
      "[335/600] train_loss: 3.87829 valid_loss: 4.17905\n",
      "Validation loss decreased (4.179403 --> 4.179047).  Saving model ...\n",
      "[336/600] train_loss: 3.87771 valid_loss: 4.17860\n",
      "Validation loss decreased (4.179047 --> 4.178595).  Saving model ...\n",
      "[337/600] train_loss: 3.87719 valid_loss: 4.17831\n",
      "Validation loss decreased (4.178595 --> 4.178309).  Saving model ...\n",
      "[338/600] train_loss: 3.87668 valid_loss: 4.17782\n",
      "Validation loss decreased (4.178309 --> 4.177819).  Saving model ...\n",
      "[339/600] train_loss: 3.87611 valid_loss: 4.17747\n",
      "Validation loss decreased (4.177819 --> 4.177473).  Saving model ...\n",
      "[340/600] train_loss: 3.87545 valid_loss: 4.17704\n",
      "Validation loss decreased (4.177473 --> 4.177039).  Saving model ...\n",
      "[341/600] train_loss: 3.87496 valid_loss: 4.17678\n",
      "Validation loss decreased (4.177039 --> 4.176785).  Saving model ...\n",
      "[342/600] train_loss: 3.87444 valid_loss: 4.17627\n",
      "Validation loss decreased (4.176785 --> 4.176272).  Saving model ...\n",
      "[343/600] train_loss: 3.87391 valid_loss: 4.17582\n",
      "Validation loss decreased (4.176272 --> 4.175823).  Saving model ...\n",
      "[344/600] train_loss: 3.87330 valid_loss: 4.17549\n",
      "Validation loss decreased (4.175823 --> 4.175492).  Saving model ...\n",
      "[345/600] train_loss: 3.87276 valid_loss: 4.17514\n",
      "Validation loss decreased (4.175492 --> 4.175142).  Saving model ...\n",
      "[346/600] train_loss: 3.87219 valid_loss: 4.17472\n",
      "Validation loss decreased (4.175142 --> 4.174716).  Saving model ...\n",
      "[347/600] train_loss: 3.87169 valid_loss: 4.17439\n",
      "Validation loss decreased (4.174716 --> 4.174393).  Saving model ...\n",
      "[348/600] train_loss: 3.87119 valid_loss: 4.17403\n",
      "Validation loss decreased (4.174393 --> 4.174030).  Saving model ...\n",
      "[349/600] train_loss: 3.87069 valid_loss: 4.17364\n",
      "Validation loss decreased (4.174030 --> 4.173643).  Saving model ...\n",
      "[350/600] train_loss: 3.87009 valid_loss: 4.17318\n",
      "Validation loss decreased (4.173643 --> 4.173176).  Saving model ...\n",
      "[351/600] train_loss: 3.86956 valid_loss: 4.17285\n",
      "Validation loss decreased (4.173176 --> 4.172853).  Saving model ...\n",
      "[352/600] train_loss: 3.86907 valid_loss: 4.17250\n",
      "Validation loss decreased (4.172853 --> 4.172500).  Saving model ...\n",
      "[353/600] train_loss: 3.86849 valid_loss: 4.17204\n",
      "Validation loss decreased (4.172500 --> 4.172044).  Saving model ...\n",
      "[354/600] train_loss: 3.86796 valid_loss: 4.17170\n",
      "Validation loss decreased (4.172044 --> 4.171705).  Saving model ...\n",
      "[355/600] train_loss: 3.86754 valid_loss: 4.17130\n",
      "Validation loss decreased (4.171705 --> 4.171303).  Saving model ...\n",
      "[356/600] train_loss: 3.86696 valid_loss: 4.17102\n",
      "Validation loss decreased (4.171303 --> 4.171018).  Saving model ...\n",
      "[357/600] train_loss: 3.86645 valid_loss: 4.17060\n",
      "Validation loss decreased (4.171018 --> 4.170604).  Saving model ...\n",
      "[358/600] train_loss: 3.86594 valid_loss: 4.17019\n",
      "Validation loss decreased (4.170604 --> 4.170188).  Saving model ...\n",
      "[359/600] train_loss: 3.86543 valid_loss: 4.16986\n",
      "Validation loss decreased (4.170188 --> 4.169862).  Saving model ...\n",
      "[360/600] train_loss: 3.86495 valid_loss: 4.16961\n",
      "Validation loss decreased (4.169862 --> 4.169609).  Saving model ...\n",
      "[361/600] train_loss: 3.86438 valid_loss: 4.16926\n",
      "Validation loss decreased (4.169609 --> 4.169258).  Saving model ...\n",
      "[362/600] train_loss: 3.86391 valid_loss: 4.16876\n",
      "Validation loss decreased (4.169258 --> 4.168764).  Saving model ...\n",
      "[363/600] train_loss: 3.86341 valid_loss: 4.16847\n",
      "Validation loss decreased (4.168764 --> 4.168472).  Saving model ...\n",
      "[364/600] train_loss: 3.86292 valid_loss: 4.16819\n",
      "Validation loss decreased (4.168472 --> 4.168192).  Saving model ...\n",
      "[365/600] train_loss: 3.86239 valid_loss: 4.16780\n",
      "Validation loss decreased (4.168192 --> 4.167800).  Saving model ...\n",
      "[366/600] train_loss: 3.86190 valid_loss: 4.16743\n",
      "Validation loss decreased (4.167800 --> 4.167427).  Saving model ...\n",
      "[367/600] train_loss: 3.86145 valid_loss: 4.16706\n",
      "Validation loss decreased (4.167427 --> 4.167064).  Saving model ...\n",
      "[368/600] train_loss: 3.86096 valid_loss: 4.16672\n",
      "Validation loss decreased (4.167064 --> 4.166719).  Saving model ...\n",
      "[369/600] train_loss: 3.86044 valid_loss: 4.16629\n",
      "Validation loss decreased (4.166719 --> 4.166291).  Saving model ...\n",
      "[370/600] train_loss: 3.85992 valid_loss: 4.16613\n",
      "Validation loss decreased (4.166291 --> 4.166125).  Saving model ...\n",
      "[371/600] train_loss: 3.85947 valid_loss: 4.16578\n",
      "Validation loss decreased (4.166125 --> 4.165775).  Saving model ...\n",
      "[372/600] train_loss: 3.85900 valid_loss: 4.16541\n",
      "Validation loss decreased (4.165775 --> 4.165413).  Saving model ...\n",
      "[373/600] train_loss: 3.85852 valid_loss: 4.16503\n",
      "Validation loss decreased (4.165413 --> 4.165033).  Saving model ...\n",
      "[374/600] train_loss: 3.85802 valid_loss: 4.16476\n",
      "Validation loss decreased (4.165033 --> 4.164762).  Saving model ...\n",
      "[375/600] train_loss: 3.85751 valid_loss: 4.16436\n",
      "Validation loss decreased (4.164762 --> 4.164357).  Saving model ...\n",
      "[376/600] train_loss: 3.85706 valid_loss: 4.16406\n",
      "Validation loss decreased (4.164357 --> 4.164063).  Saving model ...\n",
      "[377/600] train_loss: 3.85659 valid_loss: 4.16376\n",
      "Validation loss decreased (4.164063 --> 4.163764).  Saving model ...\n",
      "[378/600] train_loss: 3.85609 valid_loss: 4.16343\n",
      "Validation loss decreased (4.163764 --> 4.163426).  Saving model ...\n",
      "[379/600] train_loss: 3.85568 valid_loss: 4.16308\n",
      "Validation loss decreased (4.163426 --> 4.163076).  Saving model ...\n",
      "[380/600] train_loss: 3.85524 valid_loss: 4.16281\n",
      "Validation loss decreased (4.163076 --> 4.162815).  Saving model ...\n",
      "[381/600] train_loss: 3.85468 valid_loss: 4.16241\n",
      "Validation loss decreased (4.162815 --> 4.162411).  Saving model ...\n",
      "[382/600] train_loss: 3.85428 valid_loss: 4.16213\n",
      "Validation loss decreased (4.162411 --> 4.162129).  Saving model ...\n",
      "[383/600] train_loss: 3.85379 valid_loss: 4.16182\n",
      "Validation loss decreased (4.162129 --> 4.161819).  Saving model ...\n",
      "[384/600] train_loss: 3.85328 valid_loss: 4.16156\n",
      "Validation loss decreased (4.161819 --> 4.161558).  Saving model ...\n",
      "[385/600] train_loss: 3.85286 valid_loss: 4.16119\n",
      "Validation loss decreased (4.161558 --> 4.161192).  Saving model ...\n",
      "[386/600] train_loss: 3.85243 valid_loss: 4.16089\n",
      "Validation loss decreased (4.161192 --> 4.160893).  Saving model ...\n",
      "[387/600] train_loss: 3.85193 valid_loss: 4.16047\n",
      "Validation loss decreased (4.160893 --> 4.160466).  Saving model ...\n",
      "[388/600] train_loss: 3.85150 valid_loss: 4.16015\n",
      "Validation loss decreased (4.160466 --> 4.160148).  Saving model ...\n",
      "[389/600] train_loss: 3.85106 valid_loss: 4.15986\n",
      "Validation loss decreased (4.160148 --> 4.159859).  Saving model ...\n",
      "[390/600] train_loss: 3.85058 valid_loss: 4.15951\n",
      "Validation loss decreased (4.159859 --> 4.159508).  Saving model ...\n",
      "[391/600] train_loss: 3.85012 valid_loss: 4.15917\n",
      "Validation loss decreased (4.159508 --> 4.159165).  Saving model ...\n",
      "[392/600] train_loss: 3.84971 valid_loss: 4.15887\n",
      "Validation loss decreased (4.159165 --> 4.158867).  Saving model ...\n",
      "[393/600] train_loss: 3.84923 valid_loss: 4.15866\n",
      "Validation loss decreased (4.158867 --> 4.158660).  Saving model ...\n",
      "[394/600] train_loss: 3.84880 valid_loss: 4.15835\n",
      "Validation loss decreased (4.158660 --> 4.158349).  Saving model ...\n",
      "[395/600] train_loss: 3.84836 valid_loss: 4.15806\n",
      "Validation loss decreased (4.158349 --> 4.158058).  Saving model ...\n",
      "[396/600] train_loss: 3.84797 valid_loss: 4.15775\n",
      "Validation loss decreased (4.158058 --> 4.157751).  Saving model ...\n",
      "[397/600] train_loss: 3.84746 valid_loss: 4.15746\n",
      "Validation loss decreased (4.157751 --> 4.157462).  Saving model ...\n",
      "[398/600] train_loss: 3.84700 valid_loss: 4.15712\n",
      "Validation loss decreased (4.157462 --> 4.157122).  Saving model ...\n",
      "[399/600] train_loss: 3.84668 valid_loss: 4.15683\n",
      "Validation loss decreased (4.157122 --> 4.156830).  Saving model ...\n",
      "[400/600] train_loss: 3.84617 valid_loss: 4.15655\n",
      "Validation loss decreased (4.156830 --> 4.156553).  Saving model ...\n",
      "[401/600] train_loss: 3.84578 valid_loss: 4.15627\n",
      "Validation loss decreased (4.156553 --> 4.156266).  Saving model ...\n",
      "[402/600] train_loss: 3.84535 valid_loss: 4.15595\n",
      "Validation loss decreased (4.156266 --> 4.155950).  Saving model ...\n",
      "[403/600] train_loss: 3.84488 valid_loss: 4.15574\n",
      "Validation loss decreased (4.155950 --> 4.155742).  Saving model ...\n",
      "[404/600] train_loss: 3.84446 valid_loss: 4.15535\n",
      "Validation loss decreased (4.155742 --> 4.155350).  Saving model ...\n",
      "[405/600] train_loss: 3.84404 valid_loss: 4.15512\n",
      "Validation loss decreased (4.155350 --> 4.155121).  Saving model ...\n",
      "[406/600] train_loss: 3.84363 valid_loss: 4.15485\n",
      "Validation loss decreased (4.155121 --> 4.154854).  Saving model ...\n",
      "[407/600] train_loss: 3.84318 valid_loss: 4.15453\n",
      "Validation loss decreased (4.154854 --> 4.154530).  Saving model ...\n",
      "[408/600] train_loss: 3.84282 valid_loss: 4.15416\n",
      "Validation loss decreased (4.154530 --> 4.154160).  Saving model ...\n",
      "[409/600] train_loss: 3.84235 valid_loss: 4.15391\n",
      "Validation loss decreased (4.154160 --> 4.153908).  Saving model ...\n",
      "[410/600] train_loss: 3.84195 valid_loss: 4.15370\n",
      "Validation loss decreased (4.153908 --> 4.153701).  Saving model ...\n",
      "[411/600] train_loss: 3.84151 valid_loss: 4.15336\n",
      "Validation loss decreased (4.153701 --> 4.153362).  Saving model ...\n",
      "[412/600] train_loss: 3.84110 valid_loss: 4.15306\n",
      "Validation loss decreased (4.153362 --> 4.153065).  Saving model ...\n",
      "[413/600] train_loss: 3.84069 valid_loss: 4.15293\n",
      "Validation loss decreased (4.153065 --> 4.152933).  Saving model ...\n",
      "[414/600] train_loss: 3.84027 valid_loss: 4.15248\n",
      "Validation loss decreased (4.152933 --> 4.152478).  Saving model ...\n",
      "[415/600] train_loss: 3.83988 valid_loss: 4.15233\n",
      "Validation loss decreased (4.152478 --> 4.152330).  Saving model ...\n",
      "[416/600] train_loss: 3.83948 valid_loss: 4.15195\n",
      "Validation loss decreased (4.152330 --> 4.151953).  Saving model ...\n",
      "[417/600] train_loss: 3.83904 valid_loss: 4.15167\n",
      "Validation loss decreased (4.151953 --> 4.151667).  Saving model ...\n",
      "[418/600] train_loss: 3.83868 valid_loss: 4.15142\n",
      "Validation loss decreased (4.151667 --> 4.151421).  Saving model ...\n",
      "[419/600] train_loss: 3.83825 valid_loss: 4.15119\n",
      "Validation loss decreased (4.151421 --> 4.151189).  Saving model ...\n",
      "[420/600] train_loss: 3.83780 valid_loss: 4.15091\n",
      "Validation loss decreased (4.151189 --> 4.150910).  Saving model ...\n",
      "[421/600] train_loss: 3.83746 valid_loss: 4.15060\n",
      "Validation loss decreased (4.150910 --> 4.150600).  Saving model ...\n",
      "[422/600] train_loss: 3.83703 valid_loss: 4.15036\n",
      "Validation loss decreased (4.150600 --> 4.150360).  Saving model ...\n",
      "[423/600] train_loss: 3.83665 valid_loss: 4.15018\n",
      "Validation loss decreased (4.150360 --> 4.150177).  Saving model ...\n",
      "[424/600] train_loss: 3.83623 valid_loss: 4.14981\n",
      "Validation loss decreased (4.150177 --> 4.149809).  Saving model ...\n",
      "[425/600] train_loss: 3.83580 valid_loss: 4.14952\n",
      "Validation loss decreased (4.149809 --> 4.149523).  Saving model ...\n",
      "[426/600] train_loss: 3.83545 valid_loss: 4.14932\n",
      "Validation loss decreased (4.149523 --> 4.149317).  Saving model ...\n",
      "[427/600] train_loss: 3.83506 valid_loss: 4.14900\n",
      "Validation loss decreased (4.149317 --> 4.148995).  Saving model ...\n",
      "[428/600] train_loss: 3.83467 valid_loss: 4.14879\n",
      "Validation loss decreased (4.148995 --> 4.148786).  Saving model ...\n",
      "[429/600] train_loss: 3.83425 valid_loss: 4.14849\n",
      "Validation loss decreased (4.148786 --> 4.148486).  Saving model ...\n",
      "[430/600] train_loss: 3.83394 valid_loss: 4.14829\n",
      "Validation loss decreased (4.148486 --> 4.148290).  Saving model ...\n",
      "[431/600] train_loss: 3.83351 valid_loss: 4.14801\n",
      "Validation loss decreased (4.148290 --> 4.148008).  Saving model ...\n",
      "[432/600] train_loss: 3.83315 valid_loss: 4.14780\n",
      "Validation loss decreased (4.148008 --> 4.147800).  Saving model ...\n",
      "[433/600] train_loss: 3.83275 valid_loss: 4.14748\n",
      "Validation loss decreased (4.147800 --> 4.147484).  Saving model ...\n",
      "[434/600] train_loss: 3.83232 valid_loss: 4.14730\n",
      "Validation loss decreased (4.147484 --> 4.147296).  Saving model ...\n",
      "[435/600] train_loss: 3.83199 valid_loss: 4.14702\n",
      "Validation loss decreased (4.147296 --> 4.147015).  Saving model ...\n",
      "[436/600] train_loss: 3.83163 valid_loss: 4.14676\n",
      "Validation loss decreased (4.147015 --> 4.146760).  Saving model ...\n",
      "[437/600] train_loss: 3.83121 valid_loss: 4.14643\n",
      "Validation loss decreased (4.146760 --> 4.146430).  Saving model ...\n",
      "[438/600] train_loss: 3.83081 valid_loss: 4.14628\n",
      "Validation loss decreased (4.146430 --> 4.146285).  Saving model ...\n",
      "[439/600] train_loss: 3.83045 valid_loss: 4.14597\n",
      "Validation loss decreased (4.146285 --> 4.145973).  Saving model ...\n",
      "[440/600] train_loss: 3.83011 valid_loss: 4.14575\n",
      "Validation loss decreased (4.145973 --> 4.145746).  Saving model ...\n",
      "[441/600] train_loss: 3.82967 valid_loss: 4.14553\n",
      "Validation loss decreased (4.145746 --> 4.145528).  Saving model ...\n",
      "[442/600] train_loss: 3.82928 valid_loss: 4.14514\n",
      "Validation loss decreased (4.145528 --> 4.145143).  Saving model ...\n",
      "[443/600] train_loss: 3.82895 valid_loss: 4.14500\n",
      "Validation loss decreased (4.145143 --> 4.145003).  Saving model ...\n",
      "[444/600] train_loss: 3.82865 valid_loss: 4.14483\n",
      "Validation loss decreased (4.145003 --> 4.144833).  Saving model ...\n",
      "[445/600] train_loss: 3.82822 valid_loss: 4.14448\n",
      "Validation loss decreased (4.144833 --> 4.144485).  Saving model ...\n",
      "[446/600] train_loss: 3.82793 valid_loss: 4.14430\n",
      "Validation loss decreased (4.144485 --> 4.144304).  Saving model ...\n",
      "[447/600] train_loss: 3.82748 valid_loss: 4.14402\n",
      "Validation loss decreased (4.144304 --> 4.144021).  Saving model ...\n",
      "[448/600] train_loss: 3.82711 valid_loss: 4.14390\n",
      "Validation loss decreased (4.144021 --> 4.143898).  Saving model ...\n",
      "[449/600] train_loss: 3.82675 valid_loss: 4.14352\n",
      "Validation loss decreased (4.143898 --> 4.143524).  Saving model ...\n",
      "[450/600] train_loss: 3.82644 valid_loss: 4.14326\n",
      "Validation loss decreased (4.143524 --> 4.143262).  Saving model ...\n",
      "[451/600] train_loss: 3.82599 valid_loss: 4.14306\n",
      "Validation loss decreased (4.143262 --> 4.143056).  Saving model ...\n",
      "[452/600] train_loss: 3.82567 valid_loss: 4.14283\n",
      "Validation loss decreased (4.143056 --> 4.142834).  Saving model ...\n",
      "[453/600] train_loss: 3.82529 valid_loss: 4.14267\n",
      "Validation loss decreased (4.142834 --> 4.142674).  Saving model ...\n",
      "[454/600] train_loss: 3.82495 valid_loss: 4.14230\n",
      "Validation loss decreased (4.142674 --> 4.142300).  Saving model ...\n",
      "[455/600] train_loss: 3.82457 valid_loss: 4.14215\n",
      "Validation loss decreased (4.142300 --> 4.142152).  Saving model ...\n",
      "[456/600] train_loss: 3.82426 valid_loss: 4.14192\n",
      "Validation loss decreased (4.142152 --> 4.141924).  Saving model ...\n",
      "[457/600] train_loss: 3.82387 valid_loss: 4.14165\n",
      "Validation loss decreased (4.141924 --> 4.141655).  Saving model ...\n",
      "[458/600] train_loss: 3.82350 valid_loss: 4.14140\n",
      "Validation loss decreased (4.141655 --> 4.141401).  Saving model ...\n",
      "[459/600] train_loss: 3.82318 valid_loss: 4.14117\n",
      "Validation loss decreased (4.141401 --> 4.141173).  Saving model ...\n",
      "[460/600] train_loss: 3.82283 valid_loss: 4.14107\n",
      "Validation loss decreased (4.141173 --> 4.141070).  Saving model ...\n",
      "[461/600] train_loss: 3.82250 valid_loss: 4.14080\n",
      "Validation loss decreased (4.141070 --> 4.140804).  Saving model ...\n",
      "[462/600] train_loss: 3.82215 valid_loss: 4.14052\n",
      "Validation loss decreased (4.140804 --> 4.140524).  Saving model ...\n",
      "[463/600] train_loss: 3.82182 valid_loss: 4.14031\n",
      "Validation loss decreased (4.140524 --> 4.140308).  Saving model ...\n",
      "[464/600] train_loss: 3.82137 valid_loss: 4.14018\n",
      "Validation loss decreased (4.140308 --> 4.140181).  Saving model ...\n",
      "[465/600] train_loss: 3.82106 valid_loss: 4.13990\n",
      "Validation loss decreased (4.140181 --> 4.139902).  Saving model ...\n",
      "[466/600] train_loss: 3.82082 valid_loss: 4.13963\n",
      "Validation loss decreased (4.139902 --> 4.139629).  Saving model ...\n",
      "[467/600] train_loss: 3.82047 valid_loss: 4.13944\n",
      "Validation loss decreased (4.139629 --> 4.139438).  Saving model ...\n",
      "[468/600] train_loss: 3.82002 valid_loss: 4.13920\n",
      "Validation loss decreased (4.139438 --> 4.139199).  Saving model ...\n",
      "[469/600] train_loss: 3.81971 valid_loss: 4.13895\n",
      "Validation loss decreased (4.139199 --> 4.138952).  Saving model ...\n",
      "[470/600] train_loss: 3.81943 valid_loss: 4.13880\n",
      "Validation loss decreased (4.138952 --> 4.138802).  Saving model ...\n",
      "[471/600] train_loss: 3.81901 valid_loss: 4.13856\n",
      "Validation loss decreased (4.138802 --> 4.138556).  Saving model ...\n",
      "[472/600] train_loss: 3.81868 valid_loss: 4.13834\n",
      "Validation loss decreased (4.138556 --> 4.138336).  Saving model ...\n",
      "[473/600] train_loss: 3.81836 valid_loss: 4.13798\n",
      "Validation loss decreased (4.138336 --> 4.137980).  Saving model ...\n",
      "[474/600] train_loss: 3.81802 valid_loss: 4.13790\n",
      "Validation loss decreased (4.137980 --> 4.137896).  Saving model ...\n",
      "[475/600] train_loss: 3.81769 valid_loss: 4.13768\n",
      "Validation loss decreased (4.137896 --> 4.137684).  Saving model ...\n",
      "[476/600] train_loss: 3.81737 valid_loss: 4.13748\n",
      "Validation loss decreased (4.137684 --> 4.137477).  Saving model ...\n",
      "[477/600] train_loss: 3.81707 valid_loss: 4.13726\n",
      "Validation loss decreased (4.137477 --> 4.137260).  Saving model ...\n",
      "[478/600] train_loss: 3.81668 valid_loss: 4.13714\n",
      "Validation loss decreased (4.137260 --> 4.137135).  Saving model ...\n",
      "[479/600] train_loss: 3.81639 valid_loss: 4.13686\n",
      "Validation loss decreased (4.137135 --> 4.136856).  Saving model ...\n",
      "[480/600] train_loss: 3.81605 valid_loss: 4.13665\n",
      "Validation loss decreased (4.136856 --> 4.136649).  Saving model ...\n",
      "[481/600] train_loss: 3.81568 valid_loss: 4.13637\n",
      "Validation loss decreased (4.136649 --> 4.136371).  Saving model ...\n",
      "[482/600] train_loss: 3.81536 valid_loss: 4.13619\n",
      "Validation loss decreased (4.136371 --> 4.136187).  Saving model ...\n",
      "[483/600] train_loss: 3.81498 valid_loss: 4.13593\n",
      "Validation loss decreased (4.136187 --> 4.135933).  Saving model ...\n",
      "[484/600] train_loss: 3.81474 valid_loss: 4.13578\n",
      "Validation loss decreased (4.135933 --> 4.135778).  Saving model ...\n",
      "[485/600] train_loss: 3.81444 valid_loss: 4.13559\n",
      "Validation loss decreased (4.135778 --> 4.135590).  Saving model ...\n",
      "[486/600] train_loss: 3.81408 valid_loss: 4.13532\n",
      "Validation loss decreased (4.135590 --> 4.135318).  Saving model ...\n",
      "[487/600] train_loss: 3.81375 valid_loss: 4.13523\n",
      "Validation loss decreased (4.135318 --> 4.135225).  Saving model ...\n",
      "[488/600] train_loss: 3.81348 valid_loss: 4.13495\n",
      "Validation loss decreased (4.135225 --> 4.134952).  Saving model ...\n",
      "[489/600] train_loss: 3.81312 valid_loss: 4.13475\n",
      "Validation loss decreased (4.134952 --> 4.134750).  Saving model ...\n",
      "[490/600] train_loss: 3.81274 valid_loss: 4.13458\n",
      "Validation loss decreased (4.134750 --> 4.134583).  Saving model ...\n",
      "[491/600] train_loss: 3.81250 valid_loss: 4.13439\n",
      "Validation loss decreased (4.134583 --> 4.134394).  Saving model ...\n",
      "[492/600] train_loss: 3.81218 valid_loss: 4.13416\n",
      "Validation loss decreased (4.134394 --> 4.134164).  Saving model ...\n",
      "[493/600] train_loss: 3.81188 valid_loss: 4.13388\n",
      "Validation loss decreased (4.134164 --> 4.133883).  Saving model ...\n",
      "[494/600] train_loss: 3.81158 valid_loss: 4.13383\n",
      "Validation loss decreased (4.133883 --> 4.133830).  Saving model ...\n",
      "[495/600] train_loss: 3.81117 valid_loss: 4.13352\n",
      "Validation loss decreased (4.133830 --> 4.133525).  Saving model ...\n",
      "[496/600] train_loss: 3.81093 valid_loss: 4.13324\n",
      "Validation loss decreased (4.133525 --> 4.133239).  Saving model ...\n",
      "[497/600] train_loss: 3.81061 valid_loss: 4.13316\n",
      "Validation loss decreased (4.133239 --> 4.133163).  Saving model ...\n",
      "[498/600] train_loss: 3.81029 valid_loss: 4.13297\n",
      "Validation loss decreased (4.133163 --> 4.132971).  Saving model ...\n",
      "[499/600] train_loss: 3.80997 valid_loss: 4.13275\n",
      "Validation loss decreased (4.132971 --> 4.132745).  Saving model ...\n",
      "[500/600] train_loss: 3.80965 valid_loss: 4.13259\n",
      "Validation loss decreased (4.132745 --> 4.132588).  Saving model ...\n",
      "[501/600] train_loss: 3.80935 valid_loss: 4.13244\n",
      "Validation loss decreased (4.132588 --> 4.132441).  Saving model ...\n",
      "[502/600] train_loss: 3.80901 valid_loss: 4.13216\n",
      "Validation loss decreased (4.132441 --> 4.132161).  Saving model ...\n",
      "[503/600] train_loss: 3.80875 valid_loss: 4.13198\n",
      "Validation loss decreased (4.132161 --> 4.131976).  Saving model ...\n",
      "[504/600] train_loss: 3.80840 valid_loss: 4.13182\n",
      "Validation loss decreased (4.131976 --> 4.131825).  Saving model ...\n",
      "[505/600] train_loss: 3.80814 valid_loss: 4.13150\n",
      "Validation loss decreased (4.131825 --> 4.131498).  Saving model ...\n",
      "[506/600] train_loss: 3.80786 valid_loss: 4.13138\n",
      "Validation loss decreased (4.131498 --> 4.131377).  Saving model ...\n",
      "[507/600] train_loss: 3.80749 valid_loss: 4.13123\n",
      "Validation loss decreased (4.131377 --> 4.131234).  Saving model ...\n",
      "[508/600] train_loss: 3.80719 valid_loss: 4.13110\n",
      "Validation loss decreased (4.131234 --> 4.131104).  Saving model ...\n",
      "[509/600] train_loss: 3.80689 valid_loss: 4.13093\n",
      "Validation loss decreased (4.131104 --> 4.130930).  Saving model ...\n",
      "[510/600] train_loss: 3.80665 valid_loss: 4.13061\n",
      "Validation loss decreased (4.130930 --> 4.130611).  Saving model ...\n",
      "[511/600] train_loss: 3.80634 valid_loss: 4.13048\n",
      "Validation loss decreased (4.130611 --> 4.130481).  Saving model ...\n",
      "[512/600] train_loss: 3.80608 valid_loss: 4.13025\n",
      "Validation loss decreased (4.130481 --> 4.130246).  Saving model ...\n",
      "[513/600] train_loss: 3.80567 valid_loss: 4.13010\n",
      "Validation loss decreased (4.130246 --> 4.130101).  Saving model ...\n",
      "[514/600] train_loss: 3.80543 valid_loss: 4.12992\n",
      "Validation loss decreased (4.130101 --> 4.129918).  Saving model ...\n",
      "[515/600] train_loss: 3.80511 valid_loss: 4.12972\n",
      "Validation loss decreased (4.129918 --> 4.129721).  Saving model ...\n",
      "[516/600] train_loss: 3.80484 valid_loss: 4.12961\n",
      "Validation loss decreased (4.129721 --> 4.129605).  Saving model ...\n",
      "[517/600] train_loss: 3.80452 valid_loss: 4.12936\n",
      "Validation loss decreased (4.129605 --> 4.129356).  Saving model ...\n",
      "[518/600] train_loss: 3.80427 valid_loss: 4.12920\n",
      "Validation loss decreased (4.129356 --> 4.129197).  Saving model ...\n",
      "[519/600] train_loss: 3.80397 valid_loss: 4.12896\n",
      "Validation loss decreased (4.129197 --> 4.128961).  Saving model ...\n",
      "[520/600] train_loss: 3.80369 valid_loss: 4.12876\n",
      "Validation loss decreased (4.128961 --> 4.128758).  Saving model ...\n",
      "[521/600] train_loss: 3.80332 valid_loss: 4.12866\n",
      "Validation loss decreased (4.128758 --> 4.128662).  Saving model ...\n",
      "[522/600] train_loss: 3.80313 valid_loss: 4.12846\n",
      "Validation loss decreased (4.128662 --> 4.128458).  Saving model ...\n",
      "[523/600] train_loss: 3.80276 valid_loss: 4.12830\n",
      "Validation loss decreased (4.128458 --> 4.128304).  Saving model ...\n",
      "[524/600] train_loss: 3.80249 valid_loss: 4.12811\n",
      "Validation loss decreased (4.128304 --> 4.128110).  Saving model ...\n",
      "[525/600] train_loss: 3.80221 valid_loss: 4.12792\n",
      "Validation loss decreased (4.128110 --> 4.127922).  Saving model ...\n",
      "[526/600] train_loss: 3.80192 valid_loss: 4.12769\n",
      "Validation loss decreased (4.127922 --> 4.127691).  Saving model ...\n",
      "[527/600] train_loss: 3.80171 valid_loss: 4.12752\n",
      "Validation loss decreased (4.127691 --> 4.127522).  Saving model ...\n",
      "[528/600] train_loss: 3.80132 valid_loss: 4.12732\n",
      "Validation loss decreased (4.127522 --> 4.127319).  Saving model ...\n",
      "[529/600] train_loss: 3.80104 valid_loss: 4.12723\n",
      "Validation loss decreased (4.127319 --> 4.127230).  Saving model ...\n",
      "[530/600] train_loss: 3.80080 valid_loss: 4.12700\n",
      "Validation loss decreased (4.127230 --> 4.126997).  Saving model ...\n",
      "[531/600] train_loss: 3.80052 valid_loss: 4.12684\n",
      "Validation loss decreased (4.126997 --> 4.126839).  Saving model ...\n",
      "[532/600] train_loss: 3.80019 valid_loss: 4.12672\n",
      "Validation loss decreased (4.126839 --> 4.126716).  Saving model ...\n",
      "[533/600] train_loss: 3.79994 valid_loss: 4.12654\n",
      "Validation loss decreased (4.126716 --> 4.126542).  Saving model ...\n",
      "[534/600] train_loss: 3.79963 valid_loss: 4.12633\n",
      "Validation loss decreased (4.126542 --> 4.126326).  Saving model ...\n",
      "[535/600] train_loss: 3.79938 valid_loss: 4.12622\n",
      "Validation loss decreased (4.126326 --> 4.126217).  Saving model ...\n",
      "[536/600] train_loss: 3.79910 valid_loss: 4.12605\n",
      "Validation loss decreased (4.126217 --> 4.126052).  Saving model ...\n",
      "[537/600] train_loss: 3.79880 valid_loss: 4.12588\n",
      "Validation loss decreased (4.126052 --> 4.125881).  Saving model ...\n",
      "[538/600] train_loss: 3.79857 valid_loss: 4.12565\n",
      "Validation loss decreased (4.125881 --> 4.125654).  Saving model ...\n",
      "[539/600] train_loss: 3.79825 valid_loss: 4.12549\n",
      "Validation loss decreased (4.125654 --> 4.125488).  Saving model ...\n",
      "[540/600] train_loss: 3.79794 valid_loss: 4.12536\n",
      "Validation loss decreased (4.125488 --> 4.125364).  Saving model ...\n",
      "[541/600] train_loss: 3.79771 valid_loss: 4.12520\n",
      "Validation loss decreased (4.125364 --> 4.125197).  Saving model ...\n",
      "[542/600] train_loss: 3.79746 valid_loss: 4.12493\n",
      "Validation loss decreased (4.125197 --> 4.124927).  Saving model ...\n",
      "[543/600] train_loss: 3.79722 valid_loss: 4.12485\n",
      "Validation loss decreased (4.124927 --> 4.124854).  Saving model ...\n",
      "[544/600] train_loss: 3.79687 valid_loss: 4.12470\n",
      "Validation loss decreased (4.124854 --> 4.124698).  Saving model ...\n",
      "[545/600] train_loss: 3.79664 valid_loss: 4.12450\n",
      "Validation loss decreased (4.124698 --> 4.124501).  Saving model ...\n",
      "[546/600] train_loss: 3.79633 valid_loss: 4.12429\n",
      "Validation loss decreased (4.124501 --> 4.124294).  Saving model ...\n",
      "[547/600] train_loss: 3.79611 valid_loss: 4.12420\n",
      "Validation loss decreased (4.124294 --> 4.124196).  Saving model ...\n",
      "[548/600] train_loss: 3.79583 valid_loss: 4.12403\n",
      "Validation loss decreased (4.124196 --> 4.124030).  Saving model ...\n",
      "[549/600] train_loss: 3.79554 valid_loss: 4.12376\n",
      "Validation loss decreased (4.124030 --> 4.123763).  Saving model ...\n",
      "[550/600] train_loss: 3.79524 valid_loss: 4.12366\n",
      "Validation loss decreased (4.123763 --> 4.123659).  Saving model ...\n",
      "[551/600] train_loss: 3.79504 valid_loss: 4.12351\n",
      "Validation loss decreased (4.123659 --> 4.123508).  Saving model ...\n",
      "[552/600] train_loss: 3.79476 valid_loss: 4.12328\n",
      "Validation loss decreased (4.123508 --> 4.123277).  Saving model ...\n",
      "[553/600] train_loss: 3.79445 valid_loss: 4.12317\n",
      "Validation loss decreased (4.123277 --> 4.123167).  Saving model ...\n",
      "[554/600] train_loss: 3.79420 valid_loss: 4.12302\n",
      "Validation loss decreased (4.123167 --> 4.123020).  Saving model ...\n",
      "[555/600] train_loss: 3.79394 valid_loss: 4.12290\n",
      "Validation loss decreased (4.123020 --> 4.122904).  Saving model ...\n",
      "[556/600] train_loss: 3.79372 valid_loss: 4.12268\n",
      "Validation loss decreased (4.122904 --> 4.122679).  Saving model ...\n",
      "[557/600] train_loss: 3.79341 valid_loss: 4.12254\n",
      "Validation loss decreased (4.122679 --> 4.122543).  Saving model ...\n",
      "[558/600] train_loss: 3.79313 valid_loss: 4.12239\n",
      "Validation loss decreased (4.122543 --> 4.122386).  Saving model ...\n",
      "[559/600] train_loss: 3.79295 valid_loss: 4.12216\n",
      "Validation loss decreased (4.122386 --> 4.122164).  Saving model ...\n",
      "[560/600] train_loss: 3.79262 valid_loss: 4.12209\n",
      "Validation loss decreased (4.122164 --> 4.122088).  Saving model ...\n",
      "[561/600] train_loss: 3.79244 valid_loss: 4.12183\n",
      "Validation loss decreased (4.122088 --> 4.121832).  Saving model ...\n",
      "[562/600] train_loss: 3.79210 valid_loss: 4.12170\n",
      "Validation loss decreased (4.121832 --> 4.121701).  Saving model ...\n",
      "[563/600] train_loss: 3.79182 valid_loss: 4.12155\n",
      "Validation loss decreased (4.121701 --> 4.121546).  Saving model ...\n",
      "[564/600] train_loss: 3.79163 valid_loss: 4.12144\n",
      "Validation loss decreased (4.121546 --> 4.121440).  Saving model ...\n",
      "[565/600] train_loss: 3.79131 valid_loss: 4.12132\n",
      "Validation loss decreased (4.121440 --> 4.121324).  Saving model ...\n",
      "[566/600] train_loss: 3.79108 valid_loss: 4.12111\n",
      "Validation loss decreased (4.121324 --> 4.121112).  Saving model ...\n",
      "[567/600] train_loss: 3.79080 valid_loss: 4.12096\n",
      "Validation loss decreased (4.121112 --> 4.120955).  Saving model ...\n",
      "[568/600] train_loss: 3.79058 valid_loss: 4.12084\n",
      "Validation loss decreased (4.120955 --> 4.120837).  Saving model ...\n",
      "[569/600] train_loss: 3.79033 valid_loss: 4.12068\n",
      "Validation loss decreased (4.120837 --> 4.120677).  Saving model ...\n",
      "[570/600] train_loss: 3.79007 valid_loss: 4.12053\n",
      "Validation loss decreased (4.120677 --> 4.120531).  Saving model ...\n",
      "[571/600] train_loss: 3.78977 valid_loss: 4.12032\n",
      "Validation loss decreased (4.120531 --> 4.120316).  Saving model ...\n",
      "[572/600] train_loss: 3.78951 valid_loss: 4.12020\n",
      "Validation loss decreased (4.120316 --> 4.120203).  Saving model ...\n",
      "[573/600] train_loss: 3.78929 valid_loss: 4.12009\n",
      "Validation loss decreased (4.120203 --> 4.120086).  Saving model ...\n",
      "[574/600] train_loss: 3.78904 valid_loss: 4.11986\n",
      "Validation loss decreased (4.120086 --> 4.119865).  Saving model ...\n",
      "[575/600] train_loss: 3.78877 valid_loss: 4.11983\n",
      "Validation loss decreased (4.119865 --> 4.119826).  Saving model ...\n",
      "[576/600] train_loss: 3.78855 valid_loss: 4.11960\n",
      "Validation loss decreased (4.119826 --> 4.119597).  Saving model ...\n",
      "[577/600] train_loss: 3.78828 valid_loss: 4.11944\n",
      "Validation loss decreased (4.119597 --> 4.119444).  Saving model ...\n",
      "[578/600] train_loss: 3.78805 valid_loss: 4.11940\n",
      "Validation loss decreased (4.119444 --> 4.119403).  Saving model ...\n",
      "[579/600] train_loss: 3.78778 valid_loss: 4.11921\n",
      "Validation loss decreased (4.119403 --> 4.119214).  Saving model ...\n",
      "[580/600] train_loss: 3.78754 valid_loss: 4.11902\n",
      "Validation loss decreased (4.119214 --> 4.119017).  Saving model ...\n",
      "[581/600] train_loss: 3.78733 valid_loss: 4.11888\n",
      "Validation loss decreased (4.119017 --> 4.118878).  Saving model ...\n",
      "[582/600] train_loss: 3.78708 valid_loss: 4.11871\n",
      "Validation loss decreased (4.118878 --> 4.118707).  Saving model ...\n",
      "[583/600] train_loss: 3.78682 valid_loss: 4.11861\n",
      "Validation loss decreased (4.118707 --> 4.118608).  Saving model ...\n",
      "[584/600] train_loss: 3.78659 valid_loss: 4.11842\n",
      "Validation loss decreased (4.118608 --> 4.118416).  Saving model ...\n",
      "[585/600] train_loss: 3.78627 valid_loss: 4.11825\n",
      "Validation loss decreased (4.118416 --> 4.118253).  Saving model ...\n",
      "[586/600] train_loss: 3.78607 valid_loss: 4.11821\n",
      "Validation loss decreased (4.118253 --> 4.118209).  Saving model ...\n",
      "[587/600] train_loss: 3.78589 valid_loss: 4.11799\n",
      "Validation loss decreased (4.118209 --> 4.117985).  Saving model ...\n",
      "[588/600] train_loss: 3.78559 valid_loss: 4.11784\n",
      "Validation loss decreased (4.117985 --> 4.117842).  Saving model ...\n",
      "[589/600] train_loss: 3.78543 valid_loss: 4.11774\n",
      "Validation loss decreased (4.117842 --> 4.117742).  Saving model ...\n",
      "[590/600] train_loss: 3.78507 valid_loss: 4.11750\n",
      "Validation loss decreased (4.117742 --> 4.117501).  Saving model ...\n",
      "[591/600] train_loss: 3.78483 valid_loss: 4.11750\n",
      "Validation loss decreased (4.117501 --> 4.117497).  Saving model ...\n",
      "[592/600] train_loss: 3.78469 valid_loss: 4.11731\n",
      "Validation loss decreased (4.117497 --> 4.117309).  Saving model ...\n",
      "[593/600] train_loss: 3.78437 valid_loss: 4.11717\n",
      "Validation loss decreased (4.117309 --> 4.117169).  Saving model ...\n",
      "[594/600] train_loss: 3.78413 valid_loss: 4.11703\n",
      "Validation loss decreased (4.117169 --> 4.117034).  Saving model ...\n",
      "[595/600] train_loss: 3.78392 valid_loss: 4.11694\n",
      "Validation loss decreased (4.117034 --> 4.116936).  Saving model ...\n",
      "[596/600] train_loss: 3.78369 valid_loss: 4.11680\n",
      "Validation loss decreased (4.116936 --> 4.116797).  Saving model ...\n",
      "[597/600] train_loss: 3.78343 valid_loss: 4.11662\n",
      "Validation loss decreased (4.116797 --> 4.116623).  Saving model ...\n",
      "[598/600] train_loss: 3.78323 valid_loss: 4.11654\n",
      "Validation loss decreased (4.116623 --> 4.116535).  Saving model ...\n",
      "[599/600] train_loss: 3.78294 valid_loss: 4.11630\n",
      "Validation loss decreased (4.116535 --> 4.116296).  Saving model ...\n",
      "[600/600] train_loss: 3.78270 valid_loss: 4.11618\n",
      "Validation loss decreased (4.116296 --> 4.116181).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# best trainable after extending the data_rate_limit..\n",
    "# use terminal cmd w/ MAC: \"jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset import LoadedDataSet\n",
    "from model import Word2Vec\n",
    "from utils import EarlyStopping\n",
    "\n",
    "# HyperParams\n",
    "dimensionSize = 400\n",
    "num_epochs = 600\n",
    "lr = 0.001\n",
    "batchSize = 150\n",
    "patience = 25\n",
    "save_name = \"smalldims.w2v\"\n",
    "verbose = True\n",
    "\n",
    "trainingDataset = LoadedDataSet(trainingPairs)\n",
    "trainingLoader = DataLoader(trainingDataset, batchSize, shuffle=True) \n",
    "validationDataset = LoadedDataSet(validationPairs)\n",
    "validationLoader = DataLoader(validationDataset, batchSize, shuffle=True) \n",
    "\n",
    "model = Word2Vec(indexes.vocabulary_size, dimensionSize)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "# Variables for Scheduler:\n",
    "mode='min'\n",
    "factor=0.1\n",
    "patience=10\n",
    "verbose=True\n",
    "threshold=0.0001\n",
    "threshold_mode='rel'\n",
    "cooldown=0\n",
    "min_lr=0\n",
    "eps=1e-08\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode, factor, patience, verbose, threshold, threshold_mode, cooldown, min_lr, eps)\n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience, verbose, save_name)\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "\n",
    "for e in range(1, num_epochs + 1):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################     \n",
    "    for i, (data, target) in enumerate(trainingLoader):          \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.forward(data, target)       \n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "  \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "    for i, (data, target) in enumerate(validationLoader):   \n",
    "        loss = model.forward(data, target)       \n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    avg_train_losses.append(np.average(train_losses))\n",
    "    avg_valid_losses.append(np.average(valid_losses))\n",
    "    \n",
    "#     scheduler.step(np.average(valid_losses))\n",
    "    \n",
    "###################################################    \n",
    "#   visual Update regarding the current epoch   #\n",
    "###################################################    \n",
    "    epoch_len = len(str(num_epochs))\n",
    "    print_msg = (f'[{e:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +\n",
    "                 f'train_loss: {np.average(train_losses):.5f} ' +\n",
    "                 f'valid_loss: {np.average(valid_losses):.5f}')\n",
    "    print(print_msg)\n",
    "    \n",
    "        ##################\n",
    "        # Early Stopping #\n",
    "        ##################\n",
    "    early_stopping(np.average(valid_losses), model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"EARLY STOPPING!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Loss and the Early Stopping CheckpointÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
    "plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = avg_valid_losses.index(min(avg_valid_losses))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(3.5, 7) # consistent scale\n",
    "plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('./SOFTMAX_trained/plots/loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING & EVALUATING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine similarity between ðŸ‡¬ðŸ‡§ and ðŸ‡ºðŸ‡¸ could not be computed.\n",
      "the cosine similarity between ðŸ… and ðŸ‡ºðŸ‡¸ could not be computed.\n",
      "the cosine similarity between ðŸ‡ºðŸ‡¸ and â¤ could not be computed.\n",
      "the cosine similarity between ðŸ‡ºðŸ‡¸ and ðŸ’¥ could not be computed.\n",
      "the cosine similarity between ðŸŽ¤ and ðŸ‡³ðŸ‡¬ could not be computed.\n",
      "the cosine similarity between ðŸ‡³ðŸ‡¬ and ðŸ“² could not be computed.\n",
      "the cosine similarity between ðŸ‘‡ and ðŸ‡³ðŸ‡¬ could not be computed.\n",
      "the cosine similarity between ðŸŽ§ and ðŸ‡³ðŸ‡¬ could not be computed.\n",
      "the cosine similarity between ðŸ‡³ðŸ‡¬ and ðŸŽ¶ could not be computed.\n",
      "the cosine similarity between ðŸ‘ and â†ª could not be computed.\n",
      "\n",
      "mein Spearman: 0.571855655199557\n",
      "sein Spearman: 0.7590552598973165\n",
      "mein MAE ist 0.38039831472298946\n",
      "sein MAE ist 0.24206827309236947\n",
      "mein MSE ist 0.19802350560456594\n",
      "sein MSE ist 0.08023220381526104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype <U4 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:248: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<evaluation.Metrics at 0x1a3bff1f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads the weights of a saved model and calculates and prints the following metrics: SpearManRank, MSE, MAE \n",
    "# look @ the data with TensorBoardX \"tensorboard --logdir runs\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from evaluation import Metrics\n",
    "\n",
    "loadedModel = torch.load(os.path.join(\"SOFTMAX_trained\", \"smalldims.w2v\"))\n",
    "Metrics(loadedModel, indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
